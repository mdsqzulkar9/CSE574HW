{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No update required !\n",
    "def initializeWeights(n_in,n_out):\n",
    "    \"\"\"\n",
    "    # initializeWeights return the random weights for Neural Network given the\n",
    "    # number of node in the input layer and output layer\n",
    "    # Input:\n",
    "    # n_in: number of nodes of the input layer\n",
    "    # n_out: number of nodes of the output layer\n",
    "    # Output:\n",
    "    # W: matrix of random initial weights with size (n_out x (n_in + 1))\"\"\"\n",
    "\n",
    "    epsilon = sqrt(6) / sqrt(n_in + n_out + 1);\n",
    "    W = (np.random.rand(n_out, n_in + 1)*2* epsilon) - epsilon;\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Updated and working fine\n",
    "def sigmoid(z):\n",
    "    \n",
    "    \"\"\"# Notice that z can be a scalar, a vector or a matrix\n",
    "    # return the sigmoid of input z\"\"\"\n",
    "    \n",
    "    return  1.0 / (1.0 + np.exp(-1.0 * z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    mat = loadmat('mnist_all.mat') #loads the MAT object as a Dictionary\n",
    "    # added by : Zulkar\n",
    "    # get all the training data: \n",
    "    print ('training data size:')\n",
    "    train0 = mat.get('train0')\n",
    "    print (train0.shape)\n",
    "    train1 = mat.get('train1')\n",
    "    print (train1.shape)\n",
    "    train2 = mat.get('train2')\n",
    "    print (train2.shape)\n",
    "    train3 = mat.get('train3')\n",
    "    print (train3.shape)\n",
    "    train4 = mat.get('train4')\n",
    "    print (train4.shape)\n",
    "    train5 = mat.get('train5')\n",
    "    print (train5.shape)\n",
    "    train6 = mat.get('train6')\n",
    "    print (train6.shape)\n",
    "    train7 = mat.get('train7')\n",
    "    print (train7.shape)\n",
    "    train8 = mat.get('train8')\n",
    "    print (train8.shape)\n",
    "    train9 = mat.get('train9')\n",
    "    print (train9.shape)\n",
    "\n",
    "    # get all the testing data: \n",
    "    print ('testing data size:')\n",
    "    test0 = mat.get('test0')\n",
    "    print (test0.shape)\n",
    "    test1 = mat.get('test1')\n",
    "    print (test1.shape)\n",
    "    test2 = mat.get('test2')\n",
    "    print (test2.shape)\n",
    "    test3 = mat.get('test3')\n",
    "    print (test3.shape)\n",
    "    test4 = mat.get('test4')\n",
    "    print (test4.shape)\n",
    "    test5 = mat.get('test5')\n",
    "    print (test5.shape)\n",
    "    test6 = mat.get('test6')\n",
    "    print (test6.shape)\n",
    "    test7 = mat.get('test7')\n",
    "    print (test7.shape)\n",
    "    test8 = mat.get('test8')\n",
    "    print (test8.shape)\n",
    "    test9 = mat.get('test9')\n",
    "    print (test9.shape)\n",
    "    \n",
    "    ## (1) Stack all training matrices into one 60000 \u0002 784 matrix. Do the same for test matrices.\n",
    "    train_mat = np.concatenate((train0, train1, train2, train3, train4, \n",
    "                            train5, train6, train7, train8, train9), \n",
    "                            axis=0)\n",
    "    \n",
    "    test_mat = np.concatenate((test0, test1, test2, test3, test4, \n",
    "                                test5, test6, test7, test8, test9), \n",
    "                                axis=0)\n",
    "    \n",
    "    # training data: this array contains the size of each individual training data subsets .. train0, train1, ...\n",
    "    shape_train = [train0.shape[0], train1.shape[0],train2.shape[0],train3.shape[0],\n",
    "             train4.shape[0], train5.shape[0],train6.shape[0],train7.shape[0],\n",
    "             train8.shape[0], train9.shape[0]]\n",
    "    shape_train = np.array(shape_train)\n",
    "    \n",
    "    # test data :  this array contains the size of each individual test data subsets .. test0, test1, ...\n",
    "    shape_test = [test0.shape[0], test1.shape[0],test2.shape[0],test3.shape[0],\n",
    "             test4.shape[0], test5.shape[0],test6.shape[0],test7.shape[0],\n",
    "             test8.shape[0], test9.shape[0]]\n",
    "    \n",
    "    return train_mat, test_mat, shape_train, shape_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_labels(shape_train, shape_test):\n",
    "    \n",
    "    \n",
    "    training_data_list = []\n",
    "    labels = 0;\n",
    "\n",
    "    for shape in shape_train:\n",
    "        array = np.ones((shape,), dtype=np.int)   \n",
    "        array = labels*array\n",
    "        training_data_list.append(array)\n",
    "        labels = labels + 1;\n",
    "\n",
    "    train_label_all = []\n",
    "    for classes in training_data_list:\n",
    "        train_label_all = np.concatenate((train_label_all, classes), axis=0)\n",
    "   \n",
    "\n",
    "    test_data_list = []\n",
    "    labels = 0;\n",
    "\n",
    "    for shape in shape_test:\n",
    "        array = np.ones((shape,), dtype=np.int)   \n",
    "        array = labels*array\n",
    "        test_data_list.append(array)\n",
    "        labels = labels + 1;\n",
    "\n",
    "    test_label_all = []\n",
    "    for classes in test_data_list:\n",
    "        test_label_all = np.concatenate((test_label_all, classes), axis=0)\n",
    "        \n",
    "    return train_label_all, test_label_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \"\"\" Input:\n",
    "     Although this function doesn't have any input, you are required to load\n",
    "     the MNIST data set from file 'mnist_all.mat'.\n",
    "\n",
    "     Output:\n",
    "     train_data: matrix of training set. Each row of train_data contains \n",
    "       feature vector of a image\n",
    "     train_label: vector of label corresponding to each image in the training\n",
    "       set\n",
    "     validation_data: matrix of training set. Each row of validation_data \n",
    "       contains feature vector of a image\n",
    "     validation_label: vector of label corresponding to each image in the \n",
    "       training set\n",
    "     test_data: matrix of training set. Each row of test_data contains \n",
    "       feature vector of a image\n",
    "     test_label: vector of label corresponding to each image in the testing\n",
    "       set\n",
    "\n",
    "     Some suggestions for preprocessing step:\n",
    "     - divide the original data set to training, validation and testing set\n",
    "           with corresponding labels\n",
    "     - convert original data set from integer to double by using double()\n",
    "           function\n",
    "     - normalize the data to [0, 1]\n",
    "     - feature selection\"\"\"\n",
    "\n",
    "\n",
    "    #Pick a reasonable size for validation data\n",
    "    validation_data_size = 10000\n",
    "\n",
    "    #Your code here\n",
    "\n",
    "    # added by: Zulkar\n",
    "    max_pixel = 255.0\n",
    "\n",
    "    train_mat, test_mat, shape_train, shape_test = load_data()\n",
    "\n",
    "\n",
    "    number_of_training_data = train_mat.shape[0]\n",
    "    input_dimension = train_mat.shape[1]\n",
    "\n",
    "    number_of_test_data = test_mat.shape[0]\n",
    "\n",
    "    #print \"train_mat:\"\n",
    "    #print train_mat\n",
    "    #print \"test_mat:\"\n",
    "    #print test_mat\n",
    "\n",
    "    ## (2) Create a 60000 length vector with true labels (digits) for each training example. Same for test data.\n",
    "        # training data : \n",
    "\n",
    "    train_label_all, test_label_all = generate_labels(shape_train, shape_test)\n",
    "    #print \"train_label_all:\"\n",
    "    #print train_label_all\n",
    "    #print \"test_label_all:\"\n",
    "    #print test_label_all\n",
    "\n",
    "    ## (3) Normalize the training matrix and test matrix so that the values are between 0 and 1.\n",
    "    # training data : \n",
    "    normalized_train_mat = (train_mat / max_pixel)\n",
    "    #print 'normalized training matrix:'\n",
    "    #print normalized_train_mat \n",
    "\n",
    "    #test data : \n",
    "    normalized_test_mat = (test_mat / max_pixel)\n",
    "    #print 'normalized_test matrix:'\n",
    "    #print normalized_test_mat \n",
    "\n",
    "\n",
    "    ## (4) Randomly split the 60000 X 784 normalized matrix into two matrices: \n",
    "    ## training matrix (50000 X 784) and \n",
    "    ## validation matrix (10000 X 784). \n",
    "    ## Make sure you split the true labels vector into two parts as well.\n",
    "\n",
    "    # marge normalized training matrix and training label :\n",
    "    new_size_train_mat_with_label = input_dimension + 1\n",
    "    train_mat_with_label = np.zeros((number_of_training_data, new_size_train_mat_with_label))\n",
    "    train_mat_with_label[:,:-1] = normalized_train_mat\n",
    "    train_mat_with_label[:,-1] = train_label_all\n",
    "    print (\"train_mat_with_label:\")\n",
    "    print (train_mat_with_label)\n",
    "    print ('matrix dimension: ')\n",
    "    print (train_mat_with_label.shape)\n",
    "\n",
    "\n",
    "    # shuffle rows randomly \n",
    "    np.random.shuffle(train_mat_with_label)\n",
    "    print (\"training matrix with label after shuffle:\")\n",
    "    print (train_mat_with_label)\n",
    "\n",
    "    # perform split :\n",
    "    size_of_training_data = number_of_training_data - validation_data_size\n",
    "\n",
    "    train_data_all_features = train_mat_with_label[:size_of_training_data,:-1]\n",
    "    train_label = train_mat_with_label[:size_of_training_data,-1]\n",
    "\n",
    "    validation_data_all_features = train_mat_with_label[size_of_training_data:number_of_training_data,:-1]\n",
    "    validation_label = train_mat_with_label[size_of_training_data:number_of_training_data,-1]\n",
    "\n",
    "    validation_label = validation_label.astype(int)\n",
    "    print (\"Training data without feature extraction:\")\n",
    "    print (train_data_all_features)\n",
    "    print (\"training data label:\")\n",
    "    print (train_label)\n",
    "    print (\"validation data without feature extraction:\")\n",
    "    print (validation_data_all_features)\n",
    "    print (\"validation label:\")\n",
    "    print (validation_label)\n",
    "\n",
    "\n",
    "    # test data : \n",
    "    test_data_all_features = normalized_test_mat\n",
    "    test_label = test_label_all.astype(int)\n",
    "    print (\"Test data without feature extraction:\")\n",
    "    print (test_data_all_features)\n",
    "    print (\"Test data labels: \")\n",
    "    print (test_label)\n",
    "\n",
    "\n",
    "    #    Feature selection : one can observe that there are many features which values are exactly the same for all data points in the training set.\n",
    "    # we can ignore those features in the pre-processing step.\n",
    "    # Observation : we can ignore the columns those have same values for all data points.\n",
    "\n",
    "    # merge all the data to get homogenous feature space for all training, test and validation data \n",
    "    all_data_all_features = np.concatenate((train_data_all_features,validation_data_all_features,test_data_all_features),axis = 0)\n",
    "\n",
    "    print (\"all data marged into a matrix: shape\")\n",
    "    #print all_data_all_features\n",
    "    print (all_data_all_features.shape)\n",
    "\n",
    "    all_data = all_data_all_features[:, all_data_all_features.sum(axis=0) > 0]\n",
    "    print (all_data.shape)\n",
    "\n",
    "    # split training , validation and tesing data: \n",
    "\n",
    "    train_data = all_data[:size_of_training_data,:]\n",
    "    print (\"training data after feature selection:\")\n",
    "    print (train_data)\n",
    "    validation_data = all_data[size_of_training_data:number_of_training_data,:]\n",
    "    print (\"validation data after feature selection:\")\n",
    "    print (validation_data)\n",
    "    test_data = all_data[number_of_training_data:,:]\n",
    "    print (\"test data after feature selection:\")\n",
    "    print (test_data)\n",
    "\n",
    "\n",
    "    # commented by : Zulkar : \n",
    "    # train_data = np.array([])\n",
    "    # train_label = np.array([])\n",
    "    # validation_data = np.array([])\n",
    "    # validation_label = np.array([])\n",
    "    # test_data = np.array([])\n",
    "    # test_label = np.array([])\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nnPredict(w1,w2,data):\n",
    "    \n",
    "    \"\"\"% nnPredict predicts the label of data given the parameter w1, w2 of Neural\n",
    "    % Network.\n",
    "\n",
    "    % Input:\n",
    "    % w1: matrix of weights of connections from input layer to hidden layers.\n",
    "    %     w1(i, j) represents the weight of connection from unit i in input \n",
    "    %     layer to unit j in hidden layer.\n",
    "    % w2: matrix of weights of connections from hidden layer to output layers.\n",
    "    %     w2(i, j) represents the weight of connection from unit i in input \n",
    "    %     layer to unit j in hidden layer.\n",
    "    % data: matrix of data. Each row of this matrix represents the feature \n",
    "    %       vector of a particular image\n",
    "       \n",
    "    % Output: \n",
    "    % label: a column vector of predicted labels\"\"\" \n",
    "    #################################################################################\n",
    "    # added by: Zulkar\n",
    "    # add bias 1 at (d+1) position of each data point \n",
    "    number_of_training_data = data.shape[0]\n",
    "    dimension_of_training_data = data.shape[1]\n",
    "    \n",
    "    # bias node is added to the training data:\n",
    "    train_data_bias = np.ones( (number_of_training_data, dimension_of_training_data+1))\n",
    "    train_data_bias[:,:-1] = data\n",
    "    \n",
    "    \n",
    "    # computing dot product between data points and weights w1\n",
    "    a_hidden = np.inner(train_data_bias,w1)\n",
    "    print (\"a_hidden: inner product between training data with bias and weight vector corresponds to hidden layer:\")\n",
    "    print (a_hidden)\n",
    "    \n",
    "    # compute threshold function (sigmoid)\n",
    "    z_hidden = sigmoid(a_hidden)\n",
    "    print (\"z_hidden: output of sigmoid function in hidden layer: \")\n",
    "    print (z_hidden)\n",
    "    print (\"z_hidden:shape\")\n",
    "    print (z_hidden.shape)\n",
    "    \n",
    "    # add bias hidden node (m+1)th to z_hidden. We set its value 1 directly\n",
    "    N1= z_hidden.shape[0]\n",
    "    N2 = z_hidden.shape[1]\n",
    "    z_hidden_bias = np.ones((N1,N2+1))\n",
    "    z_hidden_bias[:,:-1] = z_hidden\n",
    "    \n",
    "    # computing dot product between hidden layer output z_hidden_bias and weights w2\n",
    "    a_output = np.inner(z_hidden_bias,w2)\n",
    "    print (\"a_output: inner product between hidden layer output with bias and weight vector corresponds to output layer:\")\n",
    "    print (a_output.shape)\n",
    "    \n",
    "    # compute threshold function (sigmoid)\n",
    "    z_output = sigmoid(a_output)\n",
    "    print (\"z_output: output of sigmoid function in output layer: \")\n",
    "    print (z_output)\n",
    "    print (\"z_output:shape\")\n",
    "    print (z_output.shape)\n",
    "    \n",
    "    \n",
    "    # get the index of max element for each row\n",
    "    labels = np.argmax(z_output, axis=1)\n",
    "    print (\"predicted label :\")\n",
    "    print (labels)\n",
    "    print (labels.dtype)\n",
    "    ######################################################################################\n",
    "    #commented by : zulkar : 12:28am 2/29/16\n",
    "    #labels = np.array([])\n",
    "    #Your code here\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size:\n",
      "(5923, 784)\n",
      "(6742, 784)\n",
      "(5958, 784)\n",
      "(6131, 784)\n",
      "(5842, 784)\n",
      "(5421, 784)\n",
      "(5918, 784)\n",
      "(6265, 784)\n",
      "(5851, 784)\n",
      "(5949, 784)\n",
      "testing data size:\n",
      "(980, 784)\n",
      "(1135, 784)\n",
      "(1032, 784)\n",
      "(1010, 784)\n",
      "(982, 784)\n",
      "(892, 784)\n",
      "(958, 784)\n",
      "(1028, 784)\n",
      "(974, 784)\n",
      "(1009, 784)\n",
      "train_mat_with_label:\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  9.]\n",
      " [ 0.  0.  0. ...,  0.  0.  9.]\n",
      " [ 0.  0.  0. ...,  0.  0.  9.]]\n",
      "matrix dimension: \n",
      "(60000, 785)\n",
      "training matrix with label after shuffle:\n",
      "[[ 0.  0.  0. ...,  0.  0.  2.]\n",
      " [ 0.  0.  0. ...,  0.  0.  5.]\n",
      " [ 0.  0.  0. ...,  0.  0.  9.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  5.]\n",
      " [ 0.  0.  0. ...,  0.  0.  9.]\n",
      " [ 0.  0.  0. ...,  0.  0.  5.]]\n",
      "Training data without feature extraction:\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "training data label:\n",
      "[ 2.  5.  9. ...,  7.  7.  9.]\n",
      "validation data without feature extraction:\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "validation label:\n",
      "[0 7 6 ..., 5 9 5]\n",
      "Test data without feature extraction:\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Test data labels: \n",
      "[0 0 0 ..., 9 9 9]\n",
      "all data marged into a matrix: shape\n",
      "(70000, 784)\n",
      "(70000, 719)\n",
      "training data after feature selection:\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "validation data after feature selection:\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "test data after feature selection:\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label, validation_data,validation_label, test_data, test_label = preprocess();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-38-b126baf27f1b>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-b126baf27f1b>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    print n_class\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "#  Train Neural Network\n",
    "\n",
    "# set the number of nodes in input unit (not including bias unit)\n",
    "n_input = train_data.shape[1]; \n",
    "\n",
    "# set the number of nodes in hidden unit (not including bias unit)\n",
    "n_hidden = 50;\n",
    "\n",
    "# set the number of nodes in output unit\n",
    "n_class = 10;\n",
    "print n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize the weights into some random matrices\n",
    "initial_w1 = initializeWeights(n_input, n_hidden);\n",
    "initial_w2 = initializeWeights(n_hidden, n_class);\n",
    "# unroll 2 weight matrices into single column vector\n",
    "initialWeights = np.concatenate((initial_w1.flatten(), initial_w2.flatten()),0)\n",
    "# set the regularization hyper-parameter\n",
    "lambdaval = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This part is not working right now ... need to update \n",
    "\n",
    "args = (n_input, n_hidden, n_class, train_data, train_label, lambdaval)\n",
    "\n",
    "#Train Neural Network using fmin_cg or minimize from scipy,optimize module. Check documentation for a working example\n",
    "\n",
    "opts = {'maxiter' : 50}    # Preferred value.\n",
    "\n",
    "nn_params = minimize(nnObjFunction, initialWeights, jac=True, args=args,method='CG', options=opts)\n",
    "\n",
    "#In Case you want to use fmin_cg, you may have to split the nnObjectFunction to two functions nnObjFunctionVal\n",
    "#and nnObjGradient. Check documentation for this function before you proceed.\n",
    "#nn_params, cost = fmin_cg(nnObjFunctionVal, initialWeights, nnObjGradient,args = args, maxiter = 50)\n",
    "\n",
    "#Reshape nnParams from 1D vector into w1 and w2 matrices\n",
    "w1 = nn_params.x[0:n_hidden * (n_input + 1)].reshape( (n_hidden, (n_input + 1)))\n",
    "w2 = nn_params.x[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This part is working \n",
    "\n",
    "\n",
    "# To test this part we can send initial random weights to nnPredict(...) function \n",
    "#################### this part is just for testing. Don't put it in final version ##########\n",
    "w1 = initial_w1\n",
    "w2 = initial_w2\n",
    "#############################################################################################\n",
    "\n",
    "#Test the computed parameters\n",
    "#predicted_label = nnPredict(w1,w2,train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nnObjFunction(): \n",
    "# start: nnObjFunction :\n",
    "\n",
    "\"\"\"% nnObjFunction computes the value of objective function (negative log \n",
    "    %   likelihood error function with regularization) given the parameters \n",
    "    %   of Neural Networks, thetraining data, their corresponding training \n",
    "    %   labels and lambda - regularization hyper-parameter.\n",
    "\n",
    "    % Input:\n",
    "    % params: vector of weights of 2 matrices w1 (weights of connections from\n",
    "    %     input layer to hidden layer) and w2 (weights of connections from\n",
    "    %     hidden layer to output layer) where all of the weights are contained\n",
    "    %     in a single vector.\n",
    "    % n_input: number of node in input layer (not include the bias node)\n",
    "    % n_hidden: number of node in hidden layer (not include the bias node)\n",
    "    % n_class: number of node in output layer (number of classes in\n",
    "    %     classification problem\n",
    "    % training_data: matrix of training data. Each row of this matrix\n",
    "    %     represents the feature vector of a particular image\n",
    "    % training_label: the vector of truth label of training images. Each entry\n",
    "    %     in the vector represents the truth label of its corresponding image.\n",
    "    % lambda: regularization hyper-parameter. This value is used for fixing the\n",
    "    %     overfitting problem.\n",
    "       \n",
    "    % Output: \n",
    "    % obj_val: a scalar value representing value of error function\n",
    "    % obj_grad: a SINGLE vector of gradient value of error function\n",
    "    % NOTE: how to compute obj_grad\n",
    "    % Use backpropagation algorithm to compute the gradient of error function\n",
    "    % for each weights in weight matrices.\n",
    "\n",
    "    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    % reshape 'params' vector into 2 matrices of weight w1 and w2\n",
    "    % w1: matrix of weights of connections from input layer to hidden layers.\n",
    "    %     w1(i, j) represents the weight of connection from unit j in input \n",
    "    %     layer to unit i in hidden layer.\n",
    "    % w2: matrix of weights of connections from hidden layer to output layers.\n",
    "    %     w2(i, j) represents the weight of connection from unit j in hidden \n",
    "    %     layer to unit i in output layer.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Forward pass : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the feed forward pass for all the training input:\n",
    "#################################################################################\n",
    "# added by: Zulkar\n",
    "\n",
    "# Number of output nodes:\n",
    "n_class = 10;\n",
    "\n",
    "# add bias 1 at (d+1) position of each data point \n",
    "N1 = train_data.shape[0]\n",
    "N2 = train_data.shape[1]\n",
    "train_data_bias = np.ones((N1,N2+1))\n",
    "train_data_bias[:,:-1] = train_data\n",
    "print \"train_data_bias\"\n",
    "print train_data_bias.shape\n",
    "print train_data_bias\n",
    "\n",
    "print \"w1\"\n",
    "print w1.shape\n",
    "print w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_hidden.shape\n",
      "(50000, 50)\n",
      "[[-0.47  0.18  0.45 ..., -0.33 -0.54  0.18]\n",
      " [ 0.27  0.63 -0.46 ...,  0.36 -0.21  0.78]\n",
      " [ 0.2   0.67 -0.2  ..., -0.4   0.02  0.58]\n",
      " ..., \n",
      " [ 0.86  0.64  0.2  ..., -0.13  0.03  0.76]\n",
      " [ 0.32  0.67  0.31 ...,  0.08 -0.08  0.68]\n",
      " [-0.64 -0.08  0.6  ...,  0.2  -0.23 -0.23]]\n"
     ]
    }
   ],
   "source": [
    "# computing dot product between data points and weights w1 : hidden layer\n",
    "a_hidden = np.inner(train_data_bias,w1)\n",
    "print \"a_hidden.shape\"\n",
    "print a_hidden.shape\n",
    "print a_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_hidden.shape\n",
      "(50000, 50)\n",
      "[[ 0.38  0.55  0.61 ...,  0.42  0.37  0.54]\n",
      " [ 0.57  0.65  0.39 ...,  0.59  0.45  0.69]\n",
      " [ 0.55  0.66  0.45 ...,  0.4   0.51  0.64]\n",
      " ..., \n",
      " [ 0.7   0.66  0.55 ...,  0.47  0.51  0.68]\n",
      " [ 0.58  0.66  0.58 ...,  0.52  0.48  0.66]\n",
      " [ 0.35  0.48  0.65 ...,  0.55  0.44  0.44]]\n"
     ]
    }
   ],
   "source": [
    "# compute threshold function (sigmoid) : hidden Layer\n",
    "z_hidden = sigmoid(a_hidden)\n",
    "print \"z_hidden.shape\"\n",
    "print z_hidden.shape\n",
    "print z_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_hidden_bias\n",
      "(50000, 51)\n",
      "[[ 0.38  0.55  0.61 ...,  0.37  0.54  1.  ]\n",
      " [ 0.57  0.65  0.39 ...,  0.45  0.69  1.  ]\n",
      " [ 0.55  0.66  0.45 ...,  0.51  0.64  1.  ]\n",
      " ..., \n",
      " [ 0.7   0.66  0.55 ...,  0.51  0.68  1.  ]\n",
      " [ 0.58  0.66  0.58 ...,  0.48  0.66  1.  ]\n",
      " [ 0.35  0.48  0.65 ...,  0.44  0.44  1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# add bias hidden node (m+1)th to z_hidden. We set its value 1 directly\n",
    "N1 = z_hidden.shape[0]\n",
    "N2 = z_hidden.shape[1]\n",
    "z_hidden_bias = np.ones((N1,N2+1))\n",
    "z_hidden_bias[:,:-1] = z_hidden\n",
    "print \"z_hidden_bias\"\n",
    "print z_hidden_bias.shape\n",
    "print z_hidden_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_output.shape : \n",
      "(50000, 10)\n",
      "[[ 0.04  0.11 -0.17 ..., -1.5  -0.   -0.71]\n",
      " [-0.09  0.15  0.07 ..., -1.41  0.4  -0.86]\n",
      " [-0.05  0.28  0.01 ..., -1.41 -0.02 -0.53]\n",
      " ..., \n",
      " [-0.13  0.22  0.19 ..., -1.56  0.26 -0.85]\n",
      " [-0.34  0.31  0.04 ..., -1.7   0.11 -1.12]\n",
      " [-0.05  0.36  0.1  ..., -1.79  0.03 -0.58]]\n"
     ]
    }
   ],
   "source": [
    "# computing dot product between hidden layer output z_hidden_bias and weights w2\n",
    "a_output = np.inner(z_hidden_bias,w2)\n",
    "print \"a_output.shape : \"\n",
    "print a_output.shape \n",
    "print a_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_output.shape\n",
      "(50000, 10)\n",
      "[[ 0.51  0.53  0.46 ...,  0.18  0.5   0.33]\n",
      " [ 0.48  0.54  0.52 ...,  0.2   0.6   0.3 ]\n",
      " [ 0.49  0.57  0.5  ...,  0.2   0.49  0.37]\n",
      " ..., \n",
      " [ 0.47  0.55  0.55 ...,  0.17  0.56  0.3 ]\n",
      " [ 0.42  0.58  0.51 ...,  0.15  0.53  0.25]\n",
      " [ 0.49  0.59  0.53 ...,  0.14  0.51  0.36]]\n"
     ]
    }
   ],
   "source": [
    "# compute threshold function (sigmoid)  (equation 4)\n",
    "z_output = sigmoid(a_output)\n",
    "print \"z_output.shape\"\n",
    "print z_output.shape\n",
    "print z_output\n",
    "\n",
    "\n",
    "#labels = np.argmax(z_output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_of_k.shape\n",
      "(50000, 10)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zulkar/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:6: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# added : zulkar: \n",
    "# creating training_label : converting each label to a 10 dimension vector. \n",
    "one_of_k = np.zeros((N1,n_class))\n",
    "i = 0\n",
    "for index in train_label:\n",
    "    one_of_k[i,index] = 1\n",
    "    i = i + 1\n",
    "print \"one_of_k.shape\"\n",
    "print one_of_k.shape \n",
    "print one_of_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference.shape\n",
      "(50000, 10)\n",
      "difference\n",
      "[[-0.51 -0.53 -0.46 ..., -0.18 -0.5  -0.33]\n",
      " [-0.48 -0.54 -0.52 ..., -0.2  -0.6  -0.3 ]\n",
      " [-0.49 -0.57 -0.5  ..., -0.2   0.51 -0.37]\n",
      " ..., \n",
      " [-0.47 -0.55 -0.55 ..., -0.17 -0.56 -0.3 ]\n",
      " [-0.42 -0.58 -0.51 ..., -0.15 -0.53 -0.25]\n",
      " [-0.49 -0.59  0.47 ..., -0.14 -0.51 -0.36]]\n",
      "Jp\n",
      "(50000,)\n",
      "[ 1.18  1.13  0.91 ...,  1.21  1.04  0.91]\n"
     ]
    }
   ],
   "source": [
    "# compute the error function : J_p(W(1) ,W(2))  ... equation (5)\n",
    "difference =  one_of_k - z_output\n",
    "print \"difference.shape\"\n",
    "print difference.shape\n",
    "print \"difference\"\n",
    "print difference\n",
    "difference_squared = np.square(difference)\n",
    "summation_difference_squared = np.sum(difference_squared,axis = 1)\n",
    "Jp = 0.5 * summation_difference_squared\n",
    "\n",
    "print \"Jp\"\n",
    "print Jp.shape\n",
    "print Jp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J = \n",
      "1.01331097734\n"
     ]
    }
   ],
   "source": [
    "# total error of the entire dataset : equation (6)\n",
    "number_of_training_data = train_data.shape[0]\n",
    "J = sum(Jp)/number_of_training_data;  #  = 50000\n",
    "print \"J = \"\n",
    "print J\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_l_mat\n",
      "(50000, 10)\n",
      "[[-0.13 -0.13 -0.11 ..., -0.03 -0.12 -0.07]\n",
      " [-0.12 -0.13 -0.13 ..., -0.03 -0.14 -0.06]\n",
      " [-0.12 -0.14 -0.13 ..., -0.03  0.13 -0.09]\n",
      " ..., \n",
      " [-0.12 -0.14 -0.14 ..., -0.02 -0.14 -0.06]\n",
      " [-0.1  -0.14 -0.13 ..., -0.02 -0.13 -0.05]\n",
      " [-0.12 -0.14  0.12 ..., -0.02 -0.13 -0.08]]\n"
     ]
    }
   ],
   "source": [
    "# compute the lambda error for output layer: equation (9)\n",
    "lambda_l_mat = (one_of_k - z_output)*(1- z_output)*z_output\n",
    "print \"lambda_l_mat\"\n",
    "print lambda_l_mat.shape\n",
    "print lambda_l_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_del_Jp_w2\n",
      "(10, 51)\n",
      "[[ 2333.89  2684.88  2251.02  2584.01  2118.05  2313.61  1684.98  2754.42\n",
      "   1959.51  2941.4   1654.74  2914.02  2333.78  2463.05  2339.61  2499.94\n",
      "   2062.28  2283.35  2219.59  2686.54  2782.1   1723.05  2755.21  1967.88\n",
      "   1601.89  1673.18  3345.51  2077.78  2586.53  2396.95  2077.48  1940.93\n",
      "   2589.98  2444.88  2083.7   2328.87  2532.79  1584.41  2279.85  1586.06\n",
      "   2327.21  1847.8   2730.31  2414.12  2281.03  2666.39  2036.89  2236.31\n",
      "   1731.06  2676.02  4570.07]\n",
      " [ 2776.43  3114.86  2667.92  3083.78  2648.78  2881.24  1934.49  3384.66\n",
      "   2194.24  3701.86  2095.91  3533.95  2736.17  2926.94  2852.34  2967.81\n",
      "   2310.25  2762.6   2678.22  3137.64  3172.34  1903.11  3398.75  2394.04\n",
      "   1894.68  2117.74  4213.77  2425.74  3026.3   2995.19  2440.26  2364.95\n",
      "   2929.74  2918.29  2319.88  2712.24  3072.41  1858.65  2581.65  1806.7\n",
      "   2894.27  2280.53  3405.48  3024.52  2804.94  3140.76  2535.69  2561.68\n",
      "   2110.16  3283.37  5494.7 ]\n",
      " [ 2398.19  2755.39  2390.95  2670.1   2202.3   2470.19  1777.16  2849.26\n",
      "   2011.88  3120.09  1793.69  3030.47  2395.26  2501.5   2444.63  2715.71\n",
      "   2073.97  2470.48  2326.54  2740.29  2864.39  1770.74  2818.31  2127.41\n",
      "   1763.34  1818.1   3608.04  2199.31  2616.43  2448.45  2104.89  2028.82\n",
      "   2729.22  2527.83  2087.85  2459.45  2641.68  1644.86  2241.53  1620.64\n",
      "   2436.85  2034.08  2909.64  2506.49  2384.03  2784.59  2180.31  2256.91\n",
      "   1918.18  2843.88  4779.78]\n",
      " [  519.53   574.81   470.45   589.73   528.28   532.1    372.27   563.12\n",
      "    391.5    639.07   436.01   658.57   546.3    566.01   469.4    555.41\n",
      "    451.12   442.15   429.99   599.95   625.63   355.93   619.44   429.66\n",
      "    433.26   420.22   768.16   470.83   563.53   522.07   455.7    401.54\n",
      "    523.94   486.52   460.42   450.49   598.43   358.04   465.84   366.29\n",
      "    536.6    415.83   608.58   596.83   530.66   640.54   498.88   510.99\n",
      "    408.3    567.34  1021.22]\n",
      " [  496.06   707.86   641.09   641.82   530.19   618.05   424.54   730.61\n",
      "    466.8    720.25   422.91   821.     633.07   577.53   613.27   621.44\n",
      "    504.87   581.44   513.89   674.01   648.56   397.16   725.78   564.32\n",
      "    435.27   459.6    842.99   544.75   663.5    582.67   538.49   439.77\n",
      "    572.51   682.31   559.12   601.01   655.14   431.63   571.34   400.88\n",
      "    616.35   453.08   748.41   695.65   634.57   668.28   524.88   517.8\n",
      "    417.04   653.93  1188.98]\n",
      " [ 1472.37  1638.33  1439.38  1656.37  1420.17  1515.44  1104.83  1801.32\n",
      "   1192.99  1903.97  1115.05  1894.31  1516.65  1566.78  1502.09  1589.07\n",
      "   1285.29  1422.82  1352.94  1683.48  1733.23  1021.4   1849.97  1257.51\n",
      "   1095.05  1128.47  2166.98  1333.91  1644.29  1542.75  1323.53  1245.57\n",
      "   1581.88  1585.41  1315.28  1449.92  1627.49   994.78  1397.16  1020.9\n",
      "   1514.69  1144.83  1809.92  1654.95  1497.24  1749.76  1366.56  1410.42\n",
      "   1056.61  1680.05  2946.42]\n",
      " [ 3100.98  3651.96  3106.55  3432.09  2914.89  3257.47  2340.97  3790.67\n",
      "   2584.59  4061.93  2345.77  3992.18  3163.17  3200.17  3293.13  3434.97\n",
      "   2715.62  3177.35  3022.33  3455.71  3612.75  2302.35  3750.04  2704.14\n",
      "   2311.13  2386.88  4637.2   2865.41  3425.01  3282.98  2747.3   2738.57\n",
      "   3512.6   3365.47  2866.47  3162.23  3485.04  2142.87  2970.8   2079.72\n",
      "   3229.15  2559.11  3883.81  3403.    3153.55  3615.1   2894.34  2891.13\n",
      "   2367.62  3735.86  6252.91]\n",
      " [  268.84   290.06   310.59   305.25   230.61   245.36   162.13   386.26\n",
      "    266.72   342.88   150.88   403.01   335.15   292.37   278.24   263.8\n",
      "    195.75   333.16   208.52   330.29   297.06   180.16   364.69   324.37\n",
      "    141.66   216.81   445.86   263.58   350.99   269.61   267.75   224.73\n",
      "    231.4    365.26   277.08   324.06   350.9    195.5    277.24   209.15\n",
      "    268.12   181.66   378.88   311.27   314.4    315.05   249.25   298.1\n",
      "    241.21   318.68   578.87]\n",
      " [ 2631.31  2964.19  2628.9   2912.68  2493.37  2707.45  1952.62  3182.66\n",
      "   2174.49  3394.92  1976.34  3297.43  2692.94  2776.71  2715.37  2876.25\n",
      "   2288.61  2656.95  2545.69  2913.04  3109.37  1876.88  3241.35  2365.59\n",
      "   1941.84  2040.34  3866.18  2378.93  2903.45  2795.51  2364.8   2225.42\n",
      "   2820.36  2840.75  2260.28  2708.68  2892.15  1885.11  2524.25  1878.38\n",
      "   2771.68  2209.22  3287.65  2907.9   2683.8   3088.82  2453.59  2516.7\n",
      "   2043.57  3069.14  5285.13]\n",
      " [ 1125.9   1378.87  1274.76  1316.3   1128.    1296.64   866.64  1470.49\n",
      "   1015.74  1547.59   863.65  1571.69  1282.57  1224.1   1256.67  1284.46\n",
      "    999.71  1219.27  1098.65  1366.42  1354.33   832.44  1518.71  1130.98\n",
      "    854.23   933.27  1761.53  1117.55  1388.64  1235.39  1117.14  1029.97\n",
      "   1188.85  1351.64  1136.1   1243.53  1392.17   858.93  1161.76   847.37\n",
      "   1279.05   960.83  1520.51  1404.2   1264.57  1360.55  1125.7   1097.26\n",
      "    922.01  1360.09  2440.49]]\n"
     ]
    }
   ],
   "source": [
    "# compute equation (7) :  derivative of error function \n",
    "sum_del_Jp_w2 = np.zeros( (lambda_l_mat.shape[1], z_hidden_bias.shape[1]))\n",
    "\n",
    "for i in range(lambda_l_mat.shape[0]):\n",
    "    lambda_l = lambda_l_mat[i,:]\n",
    "    z_j = z_hidden_bias[i,:]\n",
    "    del_Jp_w2 = -1 * np.outer(lambda_l,z_j)\n",
    "    sum_del_Jp_w2 = sum_del_Jp_w2 + del_Jp_w2\n",
    "print \"sum_del_Jp_w2\"\n",
    "print sum_del_Jp_w2.shape\n",
    "print sum_del_Jp_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_lambda_l_into_weight_lj(lambda_l, w2_lj):\n",
    "    #sum = 0\n",
    "    sum = np.inner(lambda_l,w2_lj)\n",
    "    return sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# computing equation (12) : \n",
    "\n",
    "\n",
    "del_Jp_w1 = np.zeros((n_hidden,n_input+1))  # 5x6\n",
    "sum_del_Jp_w1 = np.zeros((n_hidden,n_input+1))\n",
    "\n",
    "print del_Jp_w1.shape\n",
    "\n",
    "for p in range(number_of_training_data):\n",
    "    sum_del_Jp_w1 = 0;\n",
    "    \n",
    "    z_j = z_hidden_bias[p,:-1]\n",
    "    \n",
    "    lambda_l = lambda_l_mat[p,:]\n",
    "    \n",
    "\n",
    "    for j in range(n_hidden):  # 5\n",
    "        w2_lj = w2[:,j]\n",
    "        for i in range(n_input+1):   # 6\n",
    "            term = compute_lambda_l_into_weight_lj(lambda_l,w2_lj)\n",
    "            del_Jp_w1[j,i] = -1.0 * (1.0 - z_j[j]) * z_j[j] * term * train_data_bias[p,i]\n",
    "    sum_del_Jp_w1 = sum_del_Jp_w1 + del_Jp_w1\n",
    "    \n",
    "print \"sum_del_jp_w1.shape\"\n",
    "print sum_del_Jp_w1.shape\n",
    "print sum_del_Jp_w1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute equation (16):\n",
    "np.set_printoptions(precision = 10)\n",
    "del_J_bar_w1 = (1.0/number_of_training_data) * ( sum_del_Jp_w1 + ( lambdaval * w1) )\n",
    "\n",
    "print \"del_J_bar_w1:\"\n",
    "print del_J_bar_w1.shape\n",
    "print del_J_bar_w1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute equation (17):\n",
    "np.set_printoptions(precision = 10)\n",
    "del_J_bar_w2 = (1.0/number_of_training_data) * ( sum_del_Jp_w2 + ( lambdaval * w2) )\n",
    "\n",
    "print \"del_J_bar_w2:\"\n",
    "print del_J_bar_w2.shape\n",
    "print del_J_bar_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute equation (15):\n",
    "J_bar = J + (    (lambdaval / (2*26)) * (   np.sum(np.square(w1))   +   np.sum(np.square(w2))  )    )\n",
    "print \"J_bar\"\n",
    "print J_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision = 10)\n",
    "#Make sure you reshape the gradient matrices to a 1D array. for instance if your gradient matrices are grad_w1 and grad_w2\n",
    "#you would use code similar to the one below to create a flat array\n",
    "obj_val = J_bar\n",
    "grad_w1 = del_J_bar_w1\n",
    "grad_w2 = del_J_bar_w2\n",
    "obj_grad = np.concatenate((grad_w1.flatten(), grad_w2.flatten()),0)\n",
    "#obj_grad = np.array([])\n",
    "\n",
    "print \"grad_w1\"\n",
    "print grad_w1.shape\n",
    "print grad_w1\n",
    "\n",
    "print \"grad_w2\"\n",
    "print grad_w2.shape\n",
    "print grad_w2\n",
    "\n",
    "print \"obj_grad\"\n",
    "print obj_grad.shape\n",
    "print obj_grad\n",
    "\n",
    "#return (obj_val,obj_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
