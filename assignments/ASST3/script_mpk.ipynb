{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "#np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \"\"\" \n",
    "     Input:\n",
    "     Although this function doesn't have any input, you are required to load\n",
    "     the MNIST data set from file 'mnist_all.mat'.\n",
    "\n",
    "     Output:\n",
    "     train_data: matrix of training set. Each row of train_data contains \n",
    "       feature vector of a image\n",
    "     train_label: vector of label corresponding to each image in the training\n",
    "       set\n",
    "     validation_data: matrix of training set. Each row of validation_data \n",
    "       contains feature vector of a image\n",
    "     validation_label: vector of label corresponding to each image in the \n",
    "       training set\n",
    "     test_data: matrix of training set. Each row of test_data contains \n",
    "       feature vector of a image\n",
    "     test_label: vector of label corresponding to each image in the testing\n",
    "       set\n",
    "    \"\"\"\n",
    "\n",
    "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
    "\n",
    "    n_feature = mat.get(\"train1\").shape[1]\n",
    "    n_sample = 0\n",
    "    for i in range(10):\n",
    "        n_sample = n_sample + mat.get(\"train\" + str(i)).shape[0]\n",
    "    n_validation = 1000\n",
    "    n_train = n_sample - 10 * n_validation\n",
    "\n",
    "    # Construct validation data\n",
    "    validation_data = np.zeros((10 * n_validation, n_feature))\n",
    "    for i in range(10):\n",
    "        validation_data[i * n_validation:(i + 1) * n_validation, :] = mat.get(\"train\" + str(i))[0:n_validation, :]\n",
    "\n",
    "    # Construct validation label\n",
    "    validation_label = np.ones((10 * n_validation, 1))\n",
    "    for i in range(10):\n",
    "        validation_label[i * n_validation:(i + 1) * n_validation, :] = i * np.ones((n_validation, 1))\n",
    "\n",
    "    # Construct training data and label\n",
    "    train_data = np.zeros((n_train, n_feature))\n",
    "    train_label = np.zeros((n_train, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"train\" + str(i)).shape[0]\n",
    "        train_data[temp:temp + size_i - n_validation, :] = mat.get(\"train\" + str(i))[n_validation:size_i, :]\n",
    "        train_label[temp:temp + size_i - n_validation, :] = i * np.ones((size_i - n_validation, 1))\n",
    "        temp = temp + size_i - n_validation\n",
    "\n",
    "    # Construct test data and label\n",
    "    n_test = 0\n",
    "    for i in range(10):\n",
    "        n_test = n_test + mat.get(\"test\" + str(i)).shape[0]\n",
    "    test_data = np.zeros((n_test, n_feature))\n",
    "    test_label = np.zeros((n_test, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"test\" + str(i)).shape[0]\n",
    "        test_data[temp:temp + size_i, :] = mat.get(\"test\" + str(i))\n",
    "        test_label[temp:temp + size_i, :] = i * np.ones((size_i, 1))\n",
    "        temp = temp + size_i\n",
    "\n",
    "    # Delete features which don't provide any useful information for classifiers\n",
    "    sigma = np.std(train_data, axis=0)\n",
    "    index = np.array([])\n",
    "    for i in range(n_feature):\n",
    "        if (sigma[i] > 0.001):\n",
    "            index = np.append(index, [i])\n",
    "    train_data = train_data[:, index.astype(int)]\n",
    "    validation_data = validation_data[:, index.astype(int)]\n",
    "    test_data = test_data[:, index.astype(int)]\n",
    "\n",
    "    # Scale data to 0 and 1\n",
    "    train_data /= 255.0\n",
    "    validation_data /= 255.0\n",
    "    test_data /= 255.0\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_of_k(labels,k):\n",
    "    # inputs : labels : the label vector that needs one of k encoding. dimension : N * 1 \n",
    "    #          k : in our case k = 10\n",
    "    \n",
    "    N = labels.shape[0]\n",
    "\n",
    "    # create an array of size N * k with all zeros\n",
    "    result1 = np.zeros( (N , k) )\n",
    "    \n",
    "    # forcing labels to be integer:\n",
    "    int_labels = labels.astype(int)\n",
    "    \n",
    "    row_index = 0\n",
    "    for index in int_labels:\n",
    "        result1[row_index,index] = 1\n",
    "        row_index = row_index + 1\n",
    "    result = result1.astype(float)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]]\n",
      "[[ 1.  0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  1.  0.]\n",
      " [ 1.  0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "cv = np.array([1,2,3,3,2,1])\n",
    "k = 4\n",
    "re = one_of_k(cv,k)\n",
    "print (re)\n",
    "train_data_bias = np.ones((6 , 5))\n",
    "train_data_bias[:,1:] = re\n",
    "print(train_data_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script for Logistic Regression\n",
    "\"\"\"\n",
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "# # number of classes\n",
    "n_class = 10\n",
    "\n",
    "# # number of training samples\n",
    "n_train = train_data.shape[0]\n",
    "\n",
    "# # number of features\n",
    "n_feature = train_data.shape[1]\n",
    "\n",
    "Y = np.zeros((n_train, n_class))\n",
    "for i in range(n_class):\n",
    "    Y[:, i] = (train_label == i).astype(int).ravel()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FOR EXTRA CREDIT ONLY\n",
    "def mlrObjFunction(params, *args):\n",
    "    \"\"\"\n",
    "    mlrObjFunction computes multi-class Logistic Regression error function and\n",
    "    its gradient.\n",
    "\n",
    "    Input:\n",
    "        initialWeights: the weight vector of size (D + 1) x 1\n",
    "        train_data: the data matrix of size N x D\n",
    "        labeli: the label vector of size N x 1 where each entry can be either 0 or 1\n",
    "                representing the label of corresponding feature vector\n",
    "\n",
    "    Output:\n",
    "        error: the scalar value of error function of multi-class logistic regression\n",
    "        error_grad: the vector of size (D+1) x 10 representing the gradient of\n",
    "                    error function\n",
    "    \"\"\"\n",
    "    train_data, Y = args\n",
    "    #print(\"Y\",Y)\n",
    "    n_data = train_data.shape[0]\n",
    "    n_feature = train_data.shape[1]\n",
    "    error = 0\n",
    "    error_grad = np.zeros((n_feature + 1, n_class))\n",
    "\n",
    "    \n",
    "    W = params.reshape((n_feature+1,n_class))                                  #(716,10)\n",
    "    #print(\"W: \", W)\n",
    "\n",
    "    # Formula 5 : Posterior_Probabilities\n",
    "#     train_data_bias = np.insert(train_data, 715, 1, axis = 1)                           #(50000,716)\n",
    "    #train_data_bias = np.insert(train_data, 0, 1, axis = 1)               #(50000,716)\n",
    "    train_data_bias = np.ones((n_data , n_feature + 1))\n",
    "    train_data_bias[:,1:] = train_data  # dim : N * D+1\n",
    "#     print(\"train_data_bias: \", train_data_bias)\n",
    "    #print(\"train_data_bias: \", train_data_bias)\n",
    "    w_dot_x = np.dot(train_data_bias,W)          #wTx                    #(50000,10)\n",
    "#     print(\"w_dot_x: \", w_dot_x)\n",
    "    #print(\"w_dot_x: \", w_dot_x)\n",
    "    exp_w_dot_x = np.exp(w_dot_x)                          #exp(wTx)               #(50000,10)\n",
    "#     print(\"sigmoid_w_dot_x.shape: \", exp_w_dot_x.shape)\n",
    "    print(\"exp_w_dot_x: \", exp_w_dot_x)\n",
    "#     print(\"sigmoid_w_dot_x: \", sigmoid_w_dot_x)\n",
    "    sum_exp_w_dot_x = np.sum(exp_w_dot_x,axis = 1)      #sum(exp(wTx))          #(50000,1)\n",
    "#     print(\"sum_sigmoid_w_dot_x.shape: \", sum_exp_w_dot_x.shape)\n",
    "#     print(\"sum_exp_w_dot_x: \", sum_exp_w_dot_x)\n",
    "#     print(\"sum_sigmoid_w_dot_x: \", sum_sigmoid_w_dot_x)\n",
    "    inv_sum_exp_w_dot_x = 1.0 / sum_exp_w_dot_x                                 #(50000,1)\n",
    "#     print(\"inv_sum_exp_w_dot_x.shape: \", inv_sum_exp_w_dot_x.shape)\n",
    "#     print(\"inv_sum_exp_w_dot_x: \", inv_sum_exp_w_dot_x)\n",
    "#     print(\"inv_sum_sigmoid_w_dot_x: \", inv_sum_sigmoid_w_dot_x)\n",
    "\n",
    "    posterior_probability = np.zeros((exp_w_dot_x.shape[0], exp_w_dot_x.shape[1]))\n",
    "\n",
    "    for i in range(exp_w_dot_x.shape[0]):        #50000\n",
    "        for k in range(exp_w_dot_x.shape[1]): \n",
    "            posterior_probability[i][k] = exp_w_dot_x[i][k] * inv_sum_exp_w_dot_x[i]\n",
    "            \n",
    "\n",
    "#     posterior_probability = np.dot(sigmoid_w_dot_x, inv_sum_sigmoid_w_dot_x) # =theta_nk  #(50000,1)\n",
    "#     print(\"posterior_probability.shape: \", posterior_probability.shape)\n",
    "#     print(\"posterior_probability: \", posterior_probability)\n",
    "    \n",
    "    \n",
    "#     print(\"posterior_probability: \", posterior_probability)\n",
    "    \n",
    "    # Formula 6 : likelihood\n",
    "    y_nk = one_of_k(Y,n_class)      #(50000,10)\n",
    "#     print(\"y_nk: \", y_nk)\n",
    "    ln_theta_nk = np.log(posterior_probability)         #(50000,10)\n",
    "#     print(\"ln_theta_nk.shape: \", ln_theta_nk.shape)\n",
    "#     print(\"ln_theta_nk: \", ln_theta_nk)\n",
    "    product_Y_nk_theta_nk = y_nk * ln_theta_nk\n",
    "#     print(\"product_Y_nk_theta_nk: \", product_Y_nk_theta_nk)\n",
    "    sumK_product_Y_nk_theta_nk = np.sum(product_Y_nk_theta_nk, axis=1)    #(50000,1)\n",
    "#     print(\"sumK_product_Y_nk_theta_nk.shape\",sumK_product_Y_nk_theta_nk.shape)\n",
    "#     print(\"sumK_product_Y_nk_theta_nk[0]\",sumK_product_Y_nk_theta_nk[0])\n",
    "#     print(\"sumK_product_Y_nk_theta_nk.shape\",sumK_product_Y_nk_theta_nk.shape)\n",
    "    sumN_sumK_product_Y_nk_theta_nk = np.sum(sumK_product_Y_nk_theta_nk, axis=0) #scalar\n",
    "#     print(\"sumN_sumK_product_Y_nk_theta_nk\",sumN_sumK_product_Y_nk_theta_nk)\n",
    "#     print(\"sumN_sumK_product_Y_nk_theta_nk.shape\",sumN_sumK_product_Y_nk_theta_nk.shape)\n",
    "    error = (-1.0/n_data) * sumN_sumK_product_Y_nk_theta_nk                      #scalar\n",
    "    print(\"error\",error)\n",
    "    \n",
    "#     power_pp_ynk = np.power(posterior_probability,y_nk) #(50000,10)\n",
    "#     print(\"power_pp_ynk: \", power_pp_ynk)\n",
    "#     k_product = np.prod(power_pp_ynk,axis = 1)          #(50000,1)\n",
    "#     print(\"k_product: \", k_product)\n",
    "#     likelihood = np.prod(k_product,axis = 0)            #scalar\n",
    "#     print(\"likelihood: \", likelihood)\n",
    "    \n",
    "    # Formula 7 : log likelihood (error)\n",
    "#     error = -1.0 * np.log(likelihood)      #scalar\n",
    "#     print(\"error: \", error)\n",
    "    \n",
    "    # Formula 8 : gradient of error function\n",
    "    \n",
    "    difference = posterior_probability - y_nk;              #(50000,10)\n",
    "    transpose_data = np.transpose(train_data_bias)         #(10,50000)\n",
    "    product = np.dot(transpose_data,difference)  #(716,10)\n",
    "#     error_grad = np.sum(product)                            #scalar\n",
    "    error_grad_temp2 = (1.0/n_data) * product\n",
    "    error_grad = error_grad_temp2.flatten()\n",
    "    print (error_grad.shape)\n",
    "    # Formula 9 : (not needed or used in the assignment)\n",
    "    \n",
    "    \n",
    "\n",
    "#     print(gradient_error_function.shape)\n",
    "#     print(gradient_error_function)\n",
    "\n",
    "    return error, error_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlrPredict(W, data):\n",
    "    \"\"\"\n",
    "     mlrObjFunction predicts the label of data given the data and parameter W\n",
    "     of Logistic Regression\n",
    "\n",
    "     Input:\n",
    "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight\n",
    "         vector of a Logistic Regression classifier.\n",
    "         X: the data matrix of size N x D\n",
    "\n",
    "     Output:\n",
    "         label: vector of size N x 1 representing the predicted label of\n",
    "         corresponding feature vector given in data matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    label = np.zeros((data.shape[0], 1))   #(50000, 1) \n",
    "    \n",
    "    # data - train_data or validation_data or test_data = N x D\n",
    "    \n",
    "    #add bias to data\n",
    "    #multiply W x data\n",
    "    #take the highest value of the 10 for each of the 50000 entries and return that index\n",
    "\n",
    "    # W        (716, 10)\n",
    "    # data     (50000, 715)\n",
    "    N = data.shape[0]\n",
    "    D = data.shape[1]\n",
    "    # added by : Zulkar\n",
    "    # bias should be added in the beginning of the vectors\n",
    "    # data_bias = np.insert(data, 715, 1, axis = 1)    #(50000, 716)\n",
    "    #data_bias = np.insert(data, 0, 1, axis = 1)    #(50000, 716)\n",
    "    data_bias = np.ones((N , D + 1))\n",
    "    data_bias[:,1:] = data  # dim : N * D+1\n",
    "    print(\"data bias\",data_bias)\n",
    "    dot_product = np.dot(data_bias,W)                      #(50000, 10)\n",
    "    exp_dot_product = np.exp(dot_product)\n",
    "    sum_exp_dot_product = np.sum(exp_dot_product,axis = 1)\n",
    "    inv_sum_exp_dot_product = 1.0 / sum_exp_dot_product\n",
    "    \n",
    "    posterior = np.zeros((exp_dot_product.shape[0], exp_dot_product.shape[1]))\n",
    "\n",
    "    for i in range(exp_dot_product.shape[0]):        #50000\n",
    "        for k in range(exp_dot_product.shape[1]): \n",
    "            posterior[i][k] = exp_dot_product[i][k] * inv_sum_exp_dot_product[i]\n",
    "    print(posterior)\n",
    "    # commented by : zulkar \n",
    "    \"\"\"\n",
    "    for i in range(dot_product.shape[0]):        #50000\n",
    "        max_value = 0.0\n",
    "        max_index = 0.0\n",
    "        \n",
    "        for k in range(dot_product.shape[1]):    #10\n",
    "            if dot_product[i][k] > max_value:\n",
    "                max_value = dot_product[i][k]\n",
    "                max_index = k\n",
    "                \n",
    "        label[i] = max_index\n",
    "    \"\"\"\n",
    "    label_temp = np.argmax(posterior, axis = 1)\n",
    "    label = np.reshape(label_temp, (N,1) )\n",
    "    \n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_w_dot_x:  [[ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " ..., \n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]]\n",
      "error 4.60517018599\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " ..., \n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]]\n",
      "error 4.60517018599\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.50289569e+10   8.50289569e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.64632606e+27   3.64632606e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.78237565e+12   4.78237565e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.29452055e+12   3.29452055e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.63033421e+10   7.63033421e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.47641581e+09   7.47641581e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629454041\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  4.44461611e+054   4.44461611e+054   8.47263341e-007 ...,\n",
      "    8.47263341e-007   8.47263341e-007   8.47263341e-007]\n",
      " [  6.44581010e+137   6.44581010e+137   4.87369172e-016 ...,\n",
      "    4.87369172e-016   4.87369172e-016   4.87369172e-016]\n",
      " [  2.50160324e+063   2.50160324e+063   9.03136736e-008 ...,\n",
      "    9.03136736e-008   9.03136736e-008   9.03136736e-008]\n",
      " ..., \n",
      " [  3.88115603e+062   3.88115603e+062   1.11088982e-007 ...,\n",
      "    1.11088982e-007   1.11088982e-007   1.11088982e-007]\n",
      " [  2.58653168e+054   2.58653168e+054   8.99792702e-007 ...,\n",
      "    8.99792702e-007   8.99792702e-007   8.99792702e-007]\n",
      " [  2.33596986e+049   2.33596986e+049   3.27050450e-006 ...,\n",
      "    3.27050450e-006   3.27050450e-006   3.27050450e-006]]\n",
      "error 1.38629436112\n",
      "(7160,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mds/anaconda/envs/py3k/lib/python3.5/site-packages/ipykernel/__main__.py:39: RuntimeWarning: overflow encountered in exp\n",
      "/Users/mds/anaconda/envs/py3k/lib/python3.5/site-packages/ipykernel/__main__.py:56: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_w_dot_x:  [[  3.31821007e+229   3.31821007e+229   3.14541061e-026 ...,\n",
      "    3.14541061e-026   3.14541061e-026   3.14541061e-026]\n",
      " [              inf               inf   4.88656669e-065 ...,\n",
      "    4.88656669e-065   4.88656669e-065   4.88656669e-065]\n",
      " [  1.87291215e+266   1.87291215e+266   2.59516355e-030 ...,\n",
      "    2.59516355e-030   2.59516355e-030   2.59516355e-030]\n",
      " ..., \n",
      " [  7.47543777e+262   7.47543777e+262   6.19181757e-030 ...,\n",
      "    6.19181757e-030   6.19181757e-030   6.19181757e-030]\n",
      " [  3.41518922e+228   3.41518922e+228   4.04947161e-026 ...,\n",
      "    4.04947161e-026   4.04947161e-026   4.04947161e-026]\n",
      " [  2.22619083e+207   2.22619083e+207   9.14917546e-024 ...,\n",
      "    9.14917546e-024   9.14917546e-024   9.14917546e-024]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mds/anaconda/envs/py3k/lib/python3.5/site-packages/ipykernel/__main__.py:69: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/mds/anaconda/envs/py3k/lib/python3.5/site-packages/ipykernel/__main__.py:72: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[              inf               inf   1.90626889e-061 ...,\n",
      "    1.90626889e-061   1.90626889e-061   1.90626889e-061]\n",
      " [              inf               inf   7.56102282e-154 ...,\n",
      "    7.56102282e-154   7.56102282e-154   7.56102282e-154]\n",
      " [              inf               inf   3.61023172e-071 ...,\n",
      "    3.61023172e-071   3.61023172e-071   3.61023172e-071]\n",
      " ..., \n",
      " [              inf               inf   2.86226530e-070 ...,\n",
      "    2.86226530e-070   2.86226530e-070   2.86226530e-070]\n",
      " [              inf               inf   3.47876157e-061 ...,\n",
      "    3.47876157e-061   3.47876157e-061   3.47876157e-061]\n",
      " [              inf               inf   1.40006280e-055 ...,\n",
      "    1.40006280e-055   1.40006280e-055   1.40006280e-055]]\n",
      "error nan\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.50289569e+10   8.50289569e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.64632606e+27   3.64632606e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.78237565e+12   4.78237565e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.29452055e+12   3.29452055e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.63033421e+10   7.63033421e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.47641581e+09   7.47641581e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629454041\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  1.34004081e+13   1.34004081e+13   6.10384742e-02 ...,   6.10384742e-02\n",
      "    6.10384742e-02   6.10384742e-02]\n",
      " [  1.26932984e+33   1.26932984e+33   8.66107042e-04 ...,   8.66107042e-04\n",
      "    8.66107042e-04   8.66107042e-04]\n",
      " [  1.69462537e+15   1.69462537e+15   3.90077308e-02 ...,   3.90077308e-02\n",
      "    3.90077308e-02   3.90077308e-02]\n",
      " ..., \n",
      " [  1.08312756e+15   1.08312756e+15   4.06568926e-02 ...,   4.06568926e-02\n",
      "    4.06568926e-02   4.06568926e-02]\n",
      " [  1.17663026e+13   1.17663026e+13   6.17772366e-02 ...,   6.17772366e-02\n",
      "    6.17772366e-02   6.17772366e-02]\n",
      " [  7.22679359e+11   7.22679359e+11   7.99692341e-02 ...,   7.99692341e-02\n",
      "    7.99692341e-02   7.99692341e-02]]\n",
      "error 1.38629438417\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  2.47685534e+11   2.47685534e+11   6.10384758e-02 ...,   6.10384758e-02\n",
      "    6.10384758e-02   6.10384758e-02]\n",
      " [  5.40503615e+28   5.40503615e+28   8.66107107e-04 ...,   8.66107107e-04\n",
      "    8.66107107e-04   8.66107107e-04]\n",
      " [  1.65321019e+13   1.65321019e+13   3.90077318e-02 ...,   3.90077318e-02\n",
      "    3.90077318e-02   3.90077318e-02]\n",
      " ..., \n",
      " [  1.12098675e+13   1.12098675e+13   4.06568937e-02 ...,   4.06568937e-02\n",
      "    4.06568937e-02   4.06568937e-02]\n",
      " [  2.21248129e+11   2.21248129e+11   6.17772383e-02 ...,   6.17772383e-02\n",
      "    6.17772383e-02   6.17772383e-02]\n",
      " [  1.96412586e+10   1.96412586e+10   7.99692361e-02 ...,   7.99692361e-02\n",
      "    7.99692361e-02   7.99692361e-02]]\n",
      "error 1.38629447637\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  1.06580740e+11   1.06580740e+11   6.10384761e-02 ...,   6.10384761e-02\n",
      "    6.10384761e-02   6.10384761e-02]\n",
      " [  6.44571518e+27   6.44571518e+27   8.66107121e-04 ...,   8.66107121e-04\n",
      "    8.66107121e-04   8.66107121e-04]\n",
      " [  6.21534942e+12   6.21534942e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  4.26737870e+12   4.26737870e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  9.55505792e+10   9.55505792e+10   6.17772387e-02 ...,   6.17772387e-02\n",
      "    6.17772387e-02   6.17772387e-02]\n",
      " [  9.16910668e+09   9.16910668e+09   7.99692365e-02 ...,   7.99692365e-02\n",
      "    7.99692365e-02   7.99692365e-02]]\n",
      "error 1.38629452436\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.91861672e+10   8.91861672e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  4.11276063e+27   4.11276063e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  5.05468146e+12   5.05468146e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.47964818e+12   3.47964818e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  8.00175073e+10   8.00175073e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.80587099e+09   7.80587099e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629453689\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.58909003e+10   8.58909003e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.74025851e+27   3.74025851e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.83866302e+12   4.83866302e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.33279841e+12   3.33279841e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.70734888e+10   7.70734888e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.54484952e+09   7.54484952e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629453966\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.52103584e+10   8.52103584e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.66597503e+27   3.66597503e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.79421413e+12   4.79421413e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.30257171e+12   3.30257171e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.64654272e+10   7.64654272e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.49082367e+09   7.49082367e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629454025\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.50672541e+10   8.50672541e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.65046901e+27   3.65046901e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.78487463e+12   4.78487463e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.29622009e+12   3.29622009e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.63375614e+10   7.63375614e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.47945782e+09   7.47945782e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629454038\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.50370475e+10   8.50370475e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.64720106e+27   3.64720106e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.78290356e+12   4.78290356e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.29487958e+12   3.29487958e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.63105712e+10   7.63105712e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.47705847e+09   7.47705847e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.3862945404\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.50306663e+10   8.50306663e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.64651093e+27   3.64651093e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.78248719e+12   4.78248719e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.29459641e+12   3.29459641e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.63048695e+10   7.63048695e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.47655160e+09   7.47655160e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629454041\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.50293181e+10   8.50293181e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.64636512e+27   3.64636512e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.78239921e+12   4.78239921e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.29453658e+12   3.29453658e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.63036648e+10   7.63036648e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.47644450e+09   7.47644450e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629454041\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.50290332e+10   8.50290332e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.64633431e+27   3.64633431e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.78238063e+12   4.78238063e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.29452393e+12   3.29452393e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.63034103e+10   7.63034103e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.47642187e+09   7.47642187e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629454041\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.50289730e+10   8.50289730e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.64632780e+27   3.64632780e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.78237670e+12   4.78237670e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.29452126e+12   3.29452126e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.63033565e+10   7.63033565e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.47641709e+09   7.47641709e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629454041\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.50289688e+10   8.50289688e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.64632735e+27   3.64632735e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.78237642e+12   4.78237642e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.29452107e+12   3.29452107e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.63033527e+10   7.63033527e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.47641675e+09   7.47641675e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629454041\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  1.34004081e+13   1.34004081e+13   6.10384742e-02 ...,   6.10384742e-02\n",
      "    6.10384742e-02   6.10384742e-02]\n",
      " [  1.26932984e+33   1.26932984e+33   8.66107042e-04 ...,   8.66107042e-04\n",
      "    8.66107042e-04   8.66107042e-04]\n",
      " [  1.69462537e+15   1.69462537e+15   3.90077308e-02 ...,   3.90077308e-02\n",
      "    3.90077308e-02   3.90077308e-02]\n",
      " ..., \n",
      " [  1.08312756e+15   1.08312756e+15   4.06568926e-02 ...,   4.06568926e-02\n",
      "    4.06568926e-02   4.06568926e-02]\n",
      " [  1.17663026e+13   1.17663026e+13   6.17772366e-02 ...,   6.17772366e-02\n",
      "    6.17772366e-02   6.17772366e-02]\n",
      " [  7.22679359e+11   7.22679359e+11   7.99692341e-02 ...,   7.99692341e-02\n",
      "    7.99692341e-02   7.99692341e-02]]\n",
      "error 1.38629438417\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  1.06743752e+12   1.06743752e+12   6.10384752e-02 ...,   6.10384752e-02\n",
      "    6.10384752e-02   6.10384752e-02]\n",
      " [  2.15136977e+30   2.15136977e+30   8.66107083e-04 ...,   8.66107083e-04\n",
      "    8.66107083e-04   8.66107083e-04]\n",
      " [  9.00240869e+13   9.00240869e+13   3.90077315e-02 ...,   3.90077315e-02\n",
      "    3.90077315e-02   3.90077315e-02]\n",
      " ..., \n",
      " [  5.97359733e+13   5.97359733e+13   4.06568933e-02 ...,   4.06568933e-02\n",
      "    4.06568933e-02   4.06568933e-02]\n",
      " [  9.47527477e+11   9.47527477e+11   6.17772377e-02 ...,   6.17772377e-02\n",
      "    6.17772377e-02   6.17772377e-02]\n",
      " [  7.35054554e+10   7.35054554e+10   7.99692353e-02 ...,   7.99692353e-02\n",
      "    7.99692353e-02   7.99692353e-02]]\n",
      "error 1.38629442463\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  2.47721722e+11   2.47721722e+11   6.10384758e-02 ...,   6.10384758e-02\n",
      "    6.10384758e-02   6.10384758e-02]\n",
      " [  5.40702780e+28   5.40702780e+28   8.66107107e-04 ...,   8.66107107e-04\n",
      "    8.66107107e-04   8.66107107e-04]\n",
      " [  1.65349040e+13   1.65349040e+13   3.90077318e-02 ...,   3.90077318e-02\n",
      "    3.90077318e-02   3.90077318e-02]\n",
      " ..., \n",
      " [  1.12117433e+13   1.12117433e+13   4.06568937e-02 ...,   4.06568937e-02\n",
      "    4.06568937e-02   4.06568937e-02]\n",
      " [  2.21280315e+11   2.21280315e+11   6.17772383e-02 ...,   6.17772383e-02\n",
      "    6.17772383e-02   6.17772383e-02]\n",
      " [  1.96438510e+10   1.96438510e+10   7.99692361e-02 ...,   7.99692361e-02\n",
      "    7.99692361e-02   7.99692361e-02]]\n",
      "error 1.38629447636\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  1.35572817e+11   1.35572817e+11   6.10384760e-02 ...,   6.10384760e-02\n",
      "    6.10384760e-02   6.10384760e-02]\n",
      " [  1.18244429e+28   1.18244429e+28   8.66107117e-04 ...,   8.66107117e-04\n",
      "    8.66107117e-04   8.66107117e-04]\n",
      " [  8.21658463e+12   8.21658463e+12   3.90077320e-02 ...,   3.90077320e-02\n",
      "    3.90077320e-02   3.90077320e-02]\n",
      " ..., \n",
      " [  5.62133622e+12   5.62133622e+12   4.06568939e-02 ...,   4.06568939e-02\n",
      "    4.06568939e-02   4.06568939e-02]\n",
      " [  1.21416481e+11   1.21416481e+11   6.17772386e-02 ...,   6.17772386e-02\n",
      "    6.17772386e-02   6.17772386e-02]\n",
      " [  1.13953080e+10   1.13953080e+10   7.99692364e-02 ...,   7.99692364e-02\n",
      "    7.99692364e-02   7.99692364e-02]]\n",
      "error 1.38629450888\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  1.04106737e+11   1.04106737e+11   6.10384761e-02 ...,   6.10384761e-02\n",
      "    6.10384761e-02   6.10384761e-02]\n",
      " [  6.07504250e+27   6.07504250e+27   8.66107121e-04 ...,   8.66107121e-04\n",
      "    8.66107121e-04   8.66107121e-04]\n",
      " [  6.04828734e+12   6.04828734e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  4.15412057e+12   4.15412057e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  9.33420454e+10   9.33420454e+10   6.17772387e-02 ...,   6.17772387e-02\n",
      "    6.17772387e-02   6.17772387e-02]\n",
      " [  8.97661322e+09   8.97661322e+09   7.99692365e-02 ...,   7.99692365e-02\n",
      "    7.99692365e-02   7.99692365e-02]]\n",
      "error 1.38629452595\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  9.28429170e+10   9.28429170e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  4.55136371e+27   4.55136371e+27   8.66107123e-04 ...,   8.66107123e-04\n",
      "    8.66107123e-04   8.66107123e-04]\n",
      " [  5.29589569e+12   5.29589569e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.64353157e+12   3.64353157e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  8.32839304e+10   8.32839304e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  8.09443853e+09   8.09443853e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629453398\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.83379408e+10   8.83379408e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  4.01483324e+27   4.01483324e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.99895256e+12   4.99895256e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.44177137e+12   3.44177137e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.92597404e+10   7.92597404e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.73877254e+09   7.73877254e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629453759\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.64504129e+10   8.64504129e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.80200604e+27   3.80200604e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.87524930e+12   4.87524930e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.35767563e+12   3.35767563e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.75733958e+10   7.75733958e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.58923630e+09   7.58923630e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629453918\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.56433421e+10   8.56433421e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.71313261e+27   3.71313261e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.82248746e+12   4.82248746e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.32179891e+12   3.32179891e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.68522989e+10   7.68522989e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.52520154e+09   7.52520154e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629453987\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.52952157e+10   8.52952157e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.67518844e+27   3.67518844e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.79975340e+12   4.79975340e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.30633879e+12   3.30633879e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.65412480e+10   7.65412480e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.49756246e+09   7.49756246e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629454018\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.51444798e+10   8.51444798e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.65883185e+27   3.65883185e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.78991434e+12   4.78991434e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.29964752e+12   3.29964752e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.64065638e+10   7.64065638e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.48559159e+09   7.48559159e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629454031\n",
      "(7160,)\n",
      "exp_w_dot_x:  [[  8.50791044e+10   8.50791044e+10   6.10384762e-02 ...,   6.10384762e-02\n",
      "    6.10384762e-02   6.10384762e-02]\n",
      " [  3.65175155e+27   3.65175155e+27   8.66107124e-04 ...,   8.66107124e-04\n",
      "    8.66107124e-04   8.66107124e-04]\n",
      " [  4.78564793e+12   4.78564793e+12   3.90077321e-02 ...,   3.90077321e-02\n",
      "    3.90077321e-02   3.90077321e-02]\n",
      " ..., \n",
      " [  3.29674600e+12   3.29674600e+12   4.06568940e-02 ...,   4.06568940e-02\n",
      "    4.06568940e-02   4.06568940e-02]\n",
      " [  7.63481499e+10   7.63481499e+10   6.17772388e-02 ...,   6.17772388e-02\n",
      "    6.17772388e-02   6.17772388e-02]\n",
      " [  7.48039909e+09   7.48039909e+09   7.99692366e-02 ...,   7.99692366e-02\n",
      "    7.99692366e-02   7.99692366e-02]]\n",
      "error 1.38629454037\n",
      "(7160,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script for Extra Credit Part\n",
    "\"\"\"\n",
    "# FOR EXTRA CREDIT ONLY\n",
    "W_b = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights_b = np.zeros((n_feature + 1, n_class))\n",
    "opts_b = {'maxiter': 100}\n",
    "\n",
    "args_b = (train_data, Y)\n",
    "nn_params = minimize(mlrObjFunction, initialWeights_b, jac=True, args=args_b, method='CG', options=opts_b)\n",
    "W_b = nn_params.x.reshape((n_feature + 1, n_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.set_printoptions(threshold=np.inf)\n",
    "# Find the accuracy on Training Dataset\n",
    "predicted_label_b = mlrPredict(W_b, train_data)\n",
    "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label_b == train_label).astype(float))) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def one_of_k(labels,k):\n",
    "#     # inputs : labels : the label vector that needs one of k encoding. dimension : N * 1 \n",
    "#     #          k : in our case k = 10\n",
    "    \n",
    "#     N = labels.shape[0]\n",
    "\n",
    "#     # create an array of size N * k with all zeros\n",
    "#     result = np.zeros( (N , k) )\n",
    "    \n",
    "#     # forcing labels to be integer:\n",
    "#     int_labels = labels.astype(int)\n",
    "    \n",
    "#     row_index = 0\n",
    "#     for index in int_labels:\n",
    "#         result[row_index,index] = 1\n",
    "#         row_index = row_index + 1\n",
    "#     return result\n",
    "\n",
    "\n",
    "\n",
    "# FOR EXTRA CREDIT ONLY\n",
    "def mlrObjFunction(params, *args):\n",
    "    \"\"\"\n",
    "    mlrObjFunction computes multi-class Logistic Regression error function and\n",
    "    its gradient.\n",
    "\n",
    "    Input:\n",
    "        initialWeights: the weight vector of size (D + 1) x 1\n",
    "        train_data: the data matrix of size N x D\n",
    "        labeli: the label vector of size N x 1 where each entry can be either 0 or 1\n",
    "                representing the label of corresponding feature vector\n",
    "\n",
    "    Output:\n",
    "        error: the scalar value of error function of multi-class logistic regression\n",
    "        error_grad: the vector of size (D+1) x 10 representing the gradient of\n",
    "                    error function\n",
    "    \"\"\"\n",
    "    n_data = train_data.shape[0]\n",
    "    n_feature = train_data.shape[1]\n",
    "    error = 0\n",
    "    error_grad = np.zeros((n_feature + 1, n_class))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    \n",
    "    # args gives access to all of these variables - train_data, n_train, n_class\n",
    "    # args = (train_data, Y)\n",
    "    # Y = ((n_train, n_class))\n",
    "    \n",
    "    \n",
    "    # code var name  = variable shape\n",
    "    \n",
    "    # initialWeights           = (D + 1) x 1   (716,1)\n",
    "    # initialWeights_b         = (D + 1) x 1   (716,10)\n",
    "    # initialWeights_transpose =  1 x (D + 1)  (1,716)\n",
    "    # train_data               =  N x D        (50000,715)\n",
    "    # train_data_bias          =  N x (D + 1)  (50000,716)\n",
    "    # w_dot_x                  =               (50000, 1)\n",
    "    # sigma_w_dot_x            =               (50000, 1)\n",
    "    # posterior_probability    =               (50000, 1)\n",
    "    # labeli                   =  N x 1\n",
    "    # error                    =  1 x 1\n",
    "    # error_grad               = (D + 1) x 10\n",
    "    \n",
    "    # N = 50000\n",
    "    # K = \n",
    "    \n",
    "    # n_train                  = 50000\n",
    "    # n_class                  = 10\n",
    "    \n",
    "        \n",
    "# target vector yn\n",
    "# feature vector xn\n",
    "# class Ck\n",
    "# element k, which equals one\n",
    "# Y is an N × K matrix (obtained using 1-of-K encoding) of target variables with elements ynk\n",
    "    \n",
    "#     Please refer to your first assignment. This is same as the \n",
    "#     1-of-K encoding that you would have done there. ynk is the value \n",
    "#     for the Y[n][k] entry in the matrix.\n",
    "    \n",
    "    \n",
    "    W = params.reshape((n_feature+1,n_class))                                  #(716,10)\n",
    "#     print(\"W: \", W)\n",
    "\n",
    "    # Formula 5 : Posterior_Probabilities\n",
    "#     train_data_bias = np.insert(train_data, 715, 1, axis = 1)                           #(50000,716)\n",
    "    train_data_bias = np.insert(train_data, 0, 1, axis = 1)               #(50000,716)\n",
    "#     print(\"train_data_bias: \", train_data_bias)\n",
    "#     print(\"train_data_bias: \", train_data_bias)\n",
    "    w_dot_x = np.dot(train_data_bias,W)          #wTx                    #(50000,10)\n",
    "#     print(\"w_dot_x: \", w_dot_x)\n",
    "#     print(\"w_dot_x: \", w_dot_x)\n",
    "    exp_w_dot_x = np.exp(w_dot_x)                          #exp(wTx)               #(50000,10)\n",
    "#     print(\"sigmoid_w_dot_x.shape: \", exp_w_dot_x.shape)\n",
    "#     print(\"exp_w_dot_x: \", exp_w_dot_x)\n",
    "#     print(\"sigmoid_w_dot_x: \", sigmoid_w_dot_x)\n",
    "    sum_exp_w_dot_x = np.sum(exp_w_dot_x,axis = 1)      #sum(exp(wTx))          #(50000,1)\n",
    "#     print(\"sum_sigmoid_w_dot_x.shape: \", sum_exp_w_dot_x.shape)\n",
    "#     print(\"sum_exp_w_dot_x: \", sum_exp_w_dot_x)\n",
    "#     print(\"sum_sigmoid_w_dot_x: \", sum_sigmoid_w_dot_x)\n",
    "    inv_sum_exp_w_dot_x = 1.0 / sum_exp_w_dot_x                                 #(50000,1)\n",
    "#     print(\"inv_sum_exp_w_dot_x.shape: \", inv_sum_exp_w_dot_x.shape)\n",
    "#     print(\"inv_sum_exp_w_dot_x: \", inv_sum_exp_w_dot_x)\n",
    "#     print(\"inv_sum_sigmoid_w_dot_x: \", inv_sum_sigmoid_w_dot_x)\n",
    "\n",
    "    posterior_probability = np.zeros((exp_w_dot_x.shape[0], exp_w_dot_x.shape[1]))\n",
    "\n",
    "    for i in range(exp_w_dot_x.shape[0]):        #50000\n",
    "        for k in range(exp_w_dot_x.shape[1]): \n",
    "            posterior_probability[i][k] = exp_w_dot_x[i][k] * inv_sum_exp_w_dot_x[i]\n",
    "            \n",
    "\n",
    "#     posterior_probability = np.dot(sigmoid_w_dot_x, inv_sum_sigmoid_w_dot_x) # =theta_nk  #(50000,1)\n",
    "#     print(\"posterior_probability.shape: \", posterior_probability.shape)\n",
    "#     print(\"posterior_probability: \", posterior_probability)\n",
    "    \n",
    "    \n",
    "#     print(\"posterior_probability: \", posterior_probability)\n",
    "    \n",
    "    # Formula 6 : likelihood\n",
    "    y_nk = one_of_k(posterior_probability,n_class)      #(50000,10)\n",
    "#     print(\"y_nk: \", y_nk)\n",
    "    ln_theta_nk = np.log(posterior_probability)         #(50000,10)\n",
    "#     print(\"ln_theta_nk.shape: \", ln_theta_nk.shape)\n",
    "#     print(\"ln_theta_nk: \", ln_theta_nk)\n",
    "    product_Y_nk_theta_nk = y_nk * ln_theta_nk\n",
    "#     print(\"product_Y_nk_theta_nk: \", product_Y_nk_theta_nk)\n",
    "    sumK_product_Y_nk_theta_nk = np.sum(product_Y_nk_theta_nk, axis=1)    #(50000,1)\n",
    "#     print(\"sumK_product_Y_nk_theta_nk.shape\",sumK_product_Y_nk_theta_nk.shape)\n",
    "#     print(\"sumK_product_Y_nk_theta_nk[0]\",sumK_product_Y_nk_theta_nk[0])\n",
    "#     print(\"sumK_product_Y_nk_theta_nk.shape\",sumK_product_Y_nk_theta_nk.shape)\n",
    "    sumN_sumK_product_Y_nk_theta_nk = np.sum(sumK_product_Y_nk_theta_nk, axis=0) #scalar\n",
    "#     print(\"sumN_sumK_product_Y_nk_theta_nk\",sumN_sumK_product_Y_nk_theta_nk)\n",
    "#     print(\"sumN_sumK_product_Y_nk_theta_nk.shape\",sumN_sumK_product_Y_nk_theta_nk.shape)\n",
    "    error = (-1.0/n_data) * sumN_sumK_product_Y_nk_theta_nk                      #scalar\n",
    "    print(\"error\",error)\n",
    "    \n",
    "#     power_pp_ynk = np.power(posterior_probability,y_nk) #(50000,10)\n",
    "#     print(\"power_pp_ynk: \", power_pp_ynk)\n",
    "#     k_product = np.prod(power_pp_ynk,axis = 1)          #(50000,1)\n",
    "#     print(\"k_product: \", k_product)\n",
    "#     likelihood = np.prod(k_product,axis = 0)            #scalar\n",
    "#     print(\"likelihood: \", likelihood)\n",
    "    \n",
    "    # Formula 7 : log likelihood (error)\n",
    "#     error = -1.0 * np.log(likelihood)      #scalar\n",
    "#     print(\"error: \", error)\n",
    "    \n",
    "    # Formula 8 : gradient of error function\n",
    "    \n",
    "    difference = posterior_probability - y_nk;              #(50000,10)\n",
    "    transpose_difference = np.transpose(difference)         #(10,50000)\n",
    "    product = np.dot(transpose_difference,train_data_bias)  #(10,716)\n",
    "#     error_grad = np.sum(product)                            #scalar\n",
    "    error_grad_temp2 = (1.0/n_data) * product\n",
    "    error_grad = error_grad_temp2.flatten()\n",
    "    \n",
    "    # Formula 9 : (not needed or used in the assignment)\n",
    "    \n",
    "    \n",
    "\n",
    "#     print(gradient_error_function.shape)\n",
    "#     print(gradient_error_function)\n",
    "\n",
    "    return error, error_grad\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Script for Extra Credit Part\n",
    "\"\"\"\n",
    "# FOR EXTRA CREDIT ONLY\n",
    "W_b = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights_b = np.zeros((n_feature + 1, n_class))\n",
    "opts_b = {'maxiter': 100}\n",
    "\n",
    "args_b = (train_data, Y)\n",
    "nn_params = minimize(mlrObjFunction, initialWeights_b, jac=True, args=args_b, method='CG', options=opts_b)\n",
    "W_b = nn_params.x.reshape((n_feature + 1, n_class))\n",
    "\n",
    "# Find the accuracy on Training Dataset\n",
    "predicted_label_b = mlrPredict(W_b, train_data)\n",
    "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label_b == train_label).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Validation Dataset\n",
    "predicted_label_b = mlrPredict(W_b, validation_data)\n",
    "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label_b == validation_label).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Testing Dataset\n",
    "predicted_label_b = mlrPredict(W_b, test_data)\n",
    "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label_b == test_label).astype(float))) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (W_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(predicted_label_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlrPredict(W, data):\n",
    "    \"\"\"\n",
    "     mlrObjFunction predicts the label of data given the data and parameter W\n",
    "     of Logistic Regression\n",
    "\n",
    "     Input:\n",
    "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight\n",
    "         vector of a Logistic Regression classifier.\n",
    "         X: the data matrix of size N x D\n",
    "\n",
    "     Output:\n",
    "         label: vector of size N x 1 representing the predicted label of\n",
    "         corresponding feature vector given in data matrix\n",
    "\n",
    "    \"\"\"\n",
    "#     label = np.zeros((data.shape[0], 1))   #(50000, 1) \n",
    "    \n",
    "#     # data - train_data or validation_data or test_data = N x D\n",
    "    \n",
    "#     #add bias to data\n",
    "#     #multiply W x data\n",
    "#     #take the highest value of the 10 for each of the 50000 entries and return that index\n",
    "\n",
    "#     # W        (716, 10)\n",
    "#     # data     (50000, 715)\n",
    "    \n",
    "#     data_bias = np.insert(data, 715, 1, axis = 1)    #(50000, 716)\n",
    "#     dot_product = np.dot(data_bias,W)                      #(50000, 10)\n",
    "     \n",
    "#     for i in range(dot_product.shape[0]):        #50000\n",
    "#         max_value = 0.0\n",
    "#         max_index = 0.0\n",
    "        \n",
    "#         for k in range(dot_product.shape[1]):    #10\n",
    "#             if dot_product[i][k] > max_value:\n",
    "#                 max_value = dot_product[i][k]\n",
    "#                 max_index = k\n",
    "                \n",
    "#         label[i] = max_index\n",
    "    \n",
    "# #     print(data_bias.shape)\n",
    "# #     print(data_bias)\n",
    "\n",
    "#     return label\n",
    "    label = np.zeros((data.shape[0], 1))   #(50000, 1) \n",
    "    \n",
    "    # data - train_data or validation_data or test_data = N x D\n",
    "    \n",
    "    #add bias to data\n",
    "    #multiply W x data\n",
    "    #take the highest value of the 10 for each of the 50000 entries and return that index\n",
    "\n",
    "    # W        (716, 10)\n",
    "    # data     (50000, 715)\n",
    "    N = data.shape[0]\n",
    "    # added by : Zulkar\n",
    "    # bias should be added in the beginning of the vectors\n",
    "    # data_bias = np.insert(data, 715, 1, axis = 1)    #(50000, 716)\n",
    "    data_bias = np.insert(data, 0, 1, axis = 1)    #(50000, 716)\n",
    "    dot_product = np.dot(data_bias,W)                      #(50000, 10)\n",
    "    exp_dot_product = np.exp(dot_product)\n",
    "    sum_exp_dot_product = np.sum(exp_dot_product,axis = 1)\n",
    "    inv_sum_exp_dot_product = 1.0 / sum_exp_dot_product\n",
    "    \n",
    "    posterior = np.zeros((exp_dot_product.shape[0], exp_dot_product.shape[1]))\n",
    "\n",
    "    for i in range(exp_dot_product.shape[0]):        #50000\n",
    "        for k in range(exp_dot_product.shape[1]): \n",
    "            posterior[i][k] = exp_dot_product[i][k] * inv_sum_exp_dot_product[i]\n",
    "    print(posterior)\n",
    "    # commented by : zulkar \n",
    "    \"\"\"\n",
    "    for i in range(dot_product.shape[0]):        #50000\n",
    "        max_value = 0.0\n",
    "        max_index = 0.0\n",
    "        \n",
    "        for k in range(dot_product.shape[1]):    #10\n",
    "            if dot_product[i][k] > max_value:\n",
    "                max_value = dot_product[i][k]\n",
    "                max_index = k\n",
    "                \n",
    "        label[i] = max_index\n",
    "    \"\"\"\n",
    "    label_temp = np.argmax(posterior, axis = 1)\n",
    "    label = np.reshape(label_temp, (N,1) )\n",
    "    \n",
    "    return label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the accuracy on Training Dataset\n",
    "predicted_label_b = mlrPredict(W_b, train_data)\n",
    "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label_b == train_label).astype(float))) + '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
