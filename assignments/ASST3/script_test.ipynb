{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \"\"\" \n",
    "     Input:\n",
    "     Although this function doesn't have any input, you are required to load\n",
    "     the MNIST data set from file 'mnist_all.mat'.\n",
    "\n",
    "     Output:\n",
    "     train_data: matrix of training set. Each row of train_data contains \n",
    "       feature vector of a image\n",
    "     train_label: vector of label corresponding to each image in the training\n",
    "       set\n",
    "     validation_data: matrix of training set. Each row of validation_data \n",
    "       contains feature vector of a image\n",
    "     validation_label: vector of label corresponding to each image in the \n",
    "       training set\n",
    "     test_data: matrix of training set. Each row of test_data contains \n",
    "       feature vector of a image\n",
    "     test_label: vector of label corresponding to each image in the testing\n",
    "       set\n",
    "    \"\"\"\n",
    "\n",
    "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
    "\n",
    "    n_feature = mat.get(\"train1\").shape[1]\n",
    "    n_sample = 0\n",
    "    for i in range(10):\n",
    "        n_sample = n_sample + mat.get(\"train\" + str(i)).shape[0]\n",
    "    n_validation = 1000\n",
    "    n_train = n_sample - 10 * n_validation\n",
    "\n",
    "    # Construct validation data\n",
    "    validation_data = np.zeros((10 * n_validation, n_feature))\n",
    "    for i in range(10):\n",
    "        validation_data[i * n_validation:(i + 1) * n_validation, :] = mat.get(\"train\" + str(i))[0:n_validation, :]\n",
    "\n",
    "    # Construct validation label\n",
    "    validation_label = np.ones((10 * n_validation, 1))\n",
    "    for i in range(10):\n",
    "        validation_label[i * n_validation:(i + 1) * n_validation, :] = i * np.ones((n_validation, 1))\n",
    "\n",
    "    # Construct training data and label\n",
    "    train_data = np.zeros((n_train, n_feature))\n",
    "    train_label = np.zeros((n_train, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"train\" + str(i)).shape[0]\n",
    "        train_data[temp:temp + size_i - n_validation, :] = mat.get(\"train\" + str(i))[n_validation:size_i, :]\n",
    "        train_label[temp:temp + size_i - n_validation, :] = i * np.ones((size_i - n_validation, 1))\n",
    "        temp = temp + size_i - n_validation\n",
    "\n",
    "    # Construct test data and label\n",
    "    n_test = 0\n",
    "    for i in range(10):\n",
    "        n_test = n_test + mat.get(\"test\" + str(i)).shape[0]\n",
    "    test_data = np.zeros((n_test, n_feature))\n",
    "    test_label = np.zeros((n_test, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"test\" + str(i)).shape[0]\n",
    "        test_data[temp:temp + size_i, :] = mat.get(\"test\" + str(i))\n",
    "        test_label[temp:temp + size_i, :] = i * np.ones((size_i, 1))\n",
    "        temp = temp + size_i\n",
    "\n",
    "    # Delete features which don't provide any useful information for classifiers\n",
    "    sigma = np.std(train_data, axis=0)\n",
    "    index = np.array([])\n",
    "    for i in range(n_feature):\n",
    "        if (sigma[i] > 0.001):\n",
    "            index = np.append(index, [i])\n",
    "    train_data = train_data[:, index.astype(int)]\n",
    "    validation_data = validation_data[:, index.astype(int)]\n",
    "    test_data = test_data[:, index.astype(int)]\n",
    "\n",
    "    # Scale data to 0 and 1\n",
    "    train_data /= 255.0\n",
    "    validation_data /= 255.0\n",
    "    test_data /= 255.0\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blrObjFunction(initialWeights, *args):\n",
    "    \"\"\"\n",
    "    blrObjFunction computes 2-class Logistic Regression error function and\n",
    "    its gradient.\n",
    "\n",
    "    Input:\n",
    "        initialWeights: the weight vector (w_k) of size (D + 1) x 1 \n",
    "        train_data: the data matrix of size N x D\n",
    "        labeli: the label vector (y_k) of size N x 1 where each entry can be either 0 or 1 representing the label of corresponding feature vector\n",
    "\n",
    "    Output: \n",
    "        error: the scalar value of error function of 2-class logistic regression\n",
    "        error_grad: the vector of size (D+1) x 1 representing the gradient of\n",
    "                    error function\n",
    "    \"\"\"\n",
    "    train_data, labeli = args\n",
    "\n",
    "    n_data = train_data.shape[0]\n",
    "    n_features = train_data.shape[1]\n",
    "    error = 0\n",
    "    error_grad = np.zeros((n_features + 1, 1))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    \n",
    "    \n",
    "    ################################# Start ######################################\n",
    "    \n",
    "    # added by : Zulkar : 4/18/16 2:23 pm \n",
    "    # add bias term at the beginning of the feature vector instead of the end. \n",
    "    train_data_with_bias = np.ones((n_data , n_features + 1))\n",
    "    train_data_with_bias[:,1:] = train_data  # dim : N * D+1\n",
    "    #print(\"train_data_with_bias:\")\n",
    "    #print(train_data_with_bias.shape)\n",
    "    \n",
    "    # compute theta_n = sigma(w.T,x_n)   \n",
    "    # Since , initialWeights dim = (D+1) * 1\n",
    "    #          train_data_with_bias dim = N * (D+1)\n",
    "    # train_data_with_bias . initialWeights will give dim = N * 1\n",
    "    \n",
    "    W = initialWeights.reshape((n_feature+1,1))\n",
    "    theta_n_temp = np.dot(train_data_with_bias,W)  # dim = N * 1\n",
    "    theta_n = sigmoid(theta_n_temp)\n",
    "    #print(\"theta_n:\")\n",
    "    #print (theta_n.shape)\n",
    "    one_minus_theta_n = 1 - theta_n  # dim : N * 1\n",
    "    \n",
    "    ln_theta_n = np.log(theta_n)   # dim : N * 1\n",
    "    \n",
    "    ln_one_minus_theta_n = np.log(one_minus_theta_n)  # dim N * 1\n",
    "    \n",
    "    y_n = labeli   # dim : N * 1\n",
    "    \n",
    "    one_minus_y_n = 1 - labeli   # dim : N * 1\n",
    "    \n",
    "    yn_ln_thetan = y_n * ln_theta_n   # dim : N * 1\n",
    "    \n",
    "    one_minus_yn_thetan = one_minus_y_n * ln_one_minus_theta_n  # dim : N * 1\n",
    "    \n",
    "    add_both_part = yn_ln_thetan + one_minus_yn_thetan  # dim : N * 1\n",
    "    \n",
    "    e_w = np.sum(add_both_part)   # scalar\n",
    "    error = (-1.0 / n_data) * e_w  # scalar\n",
    "    \n",
    "    #print (error)\n",
    "    # added by : Zulkar : 4/18/16 2:23 pm\n",
    "    ################################## end ###############################################\n",
    "     \n",
    "    # added by : Zulkar : 4/24/16 1:35 pm\n",
    "    ################################## start ###############################################\n",
    "    theta_n_minus_y_n = theta_n - y_n  # dim : N * 1\n",
    "    \n",
    "    # transpose the training data : \n",
    "    train_data_with_bias_transpose = np.transpose(train_data_with_bias)   # dim : (D+1) * N\n",
    "    \n",
    "    \n",
    "    sum_theta_n_minus_y_n_into_xn = np.dot(train_data_with_bias_transpose, theta_n_minus_y_n)  #(D+1)*N . N*1\n",
    "    \n",
    "    error_grad_temp = (1.0 / n_data) * sum_theta_n_minus_y_n_into_xn\n",
    "    error_grad = error_grad_temp.flatten()\n",
    "    #print(\"error_grad:\")\n",
    "    #print(error_grad.shape)\n",
    "    # added by : Zulkar : 4/24/16 1:35 pm\n",
    "    ################################## start ###############################################\n",
    "    \n",
    "\n",
    "    return error, error_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script for Logistic Regression\n",
    "\"\"\"\n",
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "# number of classes\n",
    "n_class = 10\n",
    "\n",
    "# number of training samples\n",
    "n_train = train_data.shape[0]\n",
    "\n",
    "# number of features\n",
    "n_feature = train_data.shape[1]\n",
    "\n",
    "Y = np.zeros((n_train, n_class))\n",
    "for i in range(n_class):\n",
    "    Y[:, i] = (train_label == i).astype(int).ravel()\n",
    "\n",
    "# Logistic Regression with Gradient Descent\n",
    "W = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights = np.zeros((n_feature + 1, 1))\n",
    "opts = {'maxiter': 100}\n",
    "for i in range(n_class):\n",
    "    labeli = Y[:, i].reshape(n_train, 1)\n",
    "    args = (train_data, labeli)\n",
    "    nn_params = minimize(blrObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)\n",
    "    W[:, i] = nn_params.x.reshape((n_feature + 1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:\n",
      "[[ -4.40467828e+00  -6.39131538e-01  -2.96596419e+00 ...,  -9.23410877e-01\n",
      "   -1.15154127e-01  -4.15241062e+00]\n",
      " [ -3.41864545e-05  -1.60701344e-05  -1.11852755e-03 ...,  -1.73259062e-05\n",
      "   -1.41197296e-06  -4.22150600e-03]\n",
      " [ -5.07780459e-04  -5.90879950e-05   8.91514625e-04 ...,  -7.26174889e-05\n",
      "   -5.26688325e-06  -9.31704968e-03]\n",
      " ..., \n",
      " [ -5.61862897e-02  -1.53383276e-04  -7.93344221e-03 ...,   5.89974684e-02\n",
      "   -9.09938128e-06   6.53657208e-02]\n",
      " [ -1.88508187e-01  -1.09719176e-04  -1.43084582e-03 ...,   4.55799235e-01\n",
      "   -6.38749671e-06  -1.26557049e-03]\n",
      " [ -4.36097800e-02  -2.39600344e-05  -3.23087827e-04 ...,   1.07248735e-01\n",
      "   -1.34473615e-06  -8.78214281e-04]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print(\"W:\")\n",
    "print(W)\n",
    "Wfile = open(\"blrW.pickle\", 'wb')\n",
    "pickle.dump(W, Wfile)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blrPredict(W, data):\n",
    "    \"\"\"\n",
    "     blrObjFunction predicts the label of data given the data and parameter W \n",
    "     of Logistic Regression\n",
    "     \n",
    "     Input:\n",
    "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight \n",
    "         vector of a Logistic Regression classifier.\n",
    "         X: the data matrix of size N x D\n",
    "         \n",
    "     Output: \n",
    "         label: vector of size N x 1 representing the predicted label of \n",
    "         corresponding feature vector given in data matrix\n",
    "\n",
    "    \"\"\"\n",
    "    label = np.zeros((data.shape[0], 1))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    # add bias term at the beginning of the feature vector instead of the end. \n",
    "    N = data.shape[0]\n",
    "    D = data.shape[1]\n",
    "    data_with_bias = np.ones(( N , D + 1))  # dim : N * (D+1)\n",
    "    data_with_bias[:,1:] = data  # dim : N * D+1\n",
    "    \n",
    "    #data_with_bias_transpose = np.transpose(data_with_bias)  # dim : (D+1) * N\n",
    "    wT_x = np.dot(data_with_bias , W) # dim :  (D+1) * N . (D + 1) x 10 = (D+1) *  10\n",
    "    \n",
    "    sigma_wT_x = sigmoid(wT_x)\n",
    "    \n",
    "    label_temp = np.argmax(sigma_wT_x, axis = 1)\n",
    "    label = np.reshape(label_temp, (N,1) )\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training set Accuracy:86.222%\n",
      "\n",
      " Validation set Accuracy:85.36%\n",
      "\n",
      " Testing set Accuracy:85.3%\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy on Training Dataset\n",
    "predicted_label = blrPredict(W, train_data)\n",
    "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label == train_label).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Validation Dataset\n",
    "predicted_label = blrPredict(W, validation_data)\n",
    "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Testing Dataset\n",
    "predicted_label = blrPredict(W, test_data)\n",
    "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label == test_label).astype(float))) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 715)\n",
      "(10000, 715)\n",
      "(10000, 715)\n",
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(validation_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_of_k(labels,k):\n",
    "    # inputs : labels : the label vector that needs one of k encoding. dimension : N * 1 \n",
    "    #          k : in our case k = 10\n",
    "    \n",
    "    N = labels.shape[0]\n",
    "\n",
    "    # create an array of size N * k with all zeros\n",
    "    result = np.zeros( (N , k) )\n",
    "    \n",
    "    # forcing labels to be integer:\n",
    "    int_labels = labels.astype(int)\n",
    "    \n",
    "    row_index = 0\n",
    "    for index in int_labels:\n",
    "        result[row_index,index] = 1\n",
    "        row_index = row_index + 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "result = one_of_k(train_label,k)\n",
    "print(\"result:\")\n",
    "print(result[49000:49100,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_data = 3\n",
    "n_features = 2\n",
    "train_data = [[1,2],[3,4],[5,6]]\n",
    "train_data_with_bias = np.ones((n_data , n_features + 1))\n",
    "train_data_with_bias[:,1:] = train_data\n",
    "train_data_with_bias = train_data_with_bias - 1\n",
    "print (train_data_with_bias )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.array([[1,2],[3,4],[5,6]])\n",
    "train_label_temp = np.array([1,2,1])\n",
    "train_label = np.reshape(train_label_temp,(train_label_temp.shape[0],1))\n",
    "print(train_data.shape)\n",
    "#print(validation_data.shape)\n",
    "#print(test_data.shape)\n",
    "print(train_label.shape)\n",
    "ccc = np.argmax(train_data, axis = 1) \n",
    "print(ccc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "testing : with small dataset \n",
    "Script for Logistic Regression\n",
    "\"\"\"\n",
    "#train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "train_data = np.array([[1,2],[3,4],[5,6],[7,8],[9,10]])\n",
    "train_label_temp = np.array([1,2,1,1,2])\n",
    "train_label = np.reshape(train_label_temp,(train_label_temp.shape[0],1))\n",
    "\n",
    "# number of classes\n",
    "n_class = 2\n",
    "\n",
    "# number of training samples\n",
    "n_train = train_data.shape[0]\n",
    "print (\"n_train:\")\n",
    "print (n_train)\n",
    "# number of features\n",
    "n_feature = train_data.shape[1]\n",
    "\n",
    "Y = np.zeros((n_train, n_class))\n",
    "for i in range(n_class):\n",
    "    Y[:, i] = (train_label == i).astype(int).ravel()\n",
    "    \n",
    "# Logistic Regression with Gradient Descent\n",
    "W = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights = np.zeros((n_feature + 1, 1))\n",
    "opts = {'maxiter': 100}\n",
    "\n",
    "labeli = Y[:, 1].reshape(n_train, 1)\n",
    "args = (train_data, labeli)\n",
    "blrObjFunction(initialWeights, *args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
