{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "# np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \"\"\" \n",
    "     Input:\n",
    "     Although this function doesn't have any input, you are required to load\n",
    "     the MNIST data set from file 'mnist_all.mat'.\n",
    "\n",
    "     Output:\n",
    "     train_data: matrix of training set. Each row of train_data contains \n",
    "       feature vector of a image\n",
    "     train_label: vector of label corresponding to each image in the training\n",
    "       set\n",
    "     validation_data: matrix of training set. Each row of validation_data \n",
    "       contains feature vector of a image\n",
    "     validation_label: vector of label corresponding to each image in the \n",
    "       training set\n",
    "     test_data: matrix of training set. Each row of test_data contains \n",
    "       feature vector of a image\n",
    "     test_label: vector of label corresponding to each image in the testing\n",
    "       set\n",
    "    \"\"\"\n",
    "\n",
    "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
    "\n",
    "    n_feature = mat.get(\"train1\").shape[1]\n",
    "    n_sample = 0\n",
    "    for i in range(10):\n",
    "        n_sample = n_sample + mat.get(\"train\" + str(i)).shape[0]\n",
    "    n_validation = 1000\n",
    "    n_train = n_sample - 10 * n_validation\n",
    "\n",
    "    # Construct validation data\n",
    "    validation_data = np.zeros((10 * n_validation, n_feature))\n",
    "    for i in range(10):\n",
    "        validation_data[i * n_validation:(i + 1) * n_validation, :] = mat.get(\"train\" + str(i))[0:n_validation, :]\n",
    "\n",
    "    # Construct validation label\n",
    "    validation_label = np.ones((10 * n_validation, 1))\n",
    "    for i in range(10):\n",
    "        validation_label[i * n_validation:(i + 1) * n_validation, :] = i * np.ones((n_validation, 1))\n",
    "\n",
    "    # Construct training data and label\n",
    "    train_data = np.zeros((n_train, n_feature))\n",
    "    train_label = np.zeros((n_train, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"train\" + str(i)).shape[0]\n",
    "        train_data[temp:temp + size_i - n_validation, :] = mat.get(\"train\" + str(i))[n_validation:size_i, :]\n",
    "        train_label[temp:temp + size_i - n_validation, :] = i * np.ones((size_i - n_validation, 1))\n",
    "        temp = temp + size_i - n_validation\n",
    "\n",
    "    # Construct test data and label\n",
    "    n_test = 0\n",
    "    for i in range(10):\n",
    "        n_test = n_test + mat.get(\"test\" + str(i)).shape[0]\n",
    "    test_data = np.zeros((n_test, n_feature))\n",
    "    test_label = np.zeros((n_test, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"test\" + str(i)).shape[0]\n",
    "        test_data[temp:temp + size_i, :] = mat.get(\"test\" + str(i))\n",
    "        test_label[temp:temp + size_i, :] = i * np.ones((size_i, 1))\n",
    "        temp = temp + size_i\n",
    "\n",
    "    # Delete features which don't provide any useful information for classifiers\n",
    "    sigma = np.std(train_data, axis=0)\n",
    "    index = np.array([])\n",
    "    for i in range(n_feature):\n",
    "        if (sigma[i] > 0.001):\n",
    "            index = np.append(index, [i])\n",
    "    train_data = train_data[:, index.astype(int)]\n",
    "    validation_data = validation_data[:, index.astype(int)]\n",
    "    test_data = test_data[:, index.astype(int)]\n",
    "\n",
    "    # Scale data to 0 and 1\n",
    "    train_data /= 255.0\n",
    "    validation_data /= 255.0\n",
    "    test_data /= 255.0\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_of_k(labels,k):\n",
    "    # inputs : labels : the label vector that needs one of k encoding. dimension : N * 1 \n",
    "    #          k : in our case k = 10\n",
    "    \n",
    "    N = labels.shape[0]\n",
    "\n",
    "    # create an array of size N * k with all zeros\n",
    "    result = np.zeros( (N , k) )\n",
    "    \n",
    "    # forcing labels to be integer:\n",
    "    int_labels = labels.astype(int)\n",
    "    \n",
    "    row_index = 0\n",
    "    for index in int_labels:\n",
    "        result[row_index,index] = 1\n",
    "        row_index = row_index + 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blrObjFunction(initialWeights, *args):\n",
    "    \"\"\"\n",
    "    blrObjFunction computes 2-class Logistic Regression error function and\n",
    "    its gradient.\n",
    "\n",
    "    Input:\n",
    "        initialWeights: the weight vector (w_k) of size (D + 1) x 1 \n",
    "        train_data: the data matrix of size N x D\n",
    "        labeli: the label vector (y_k) of size N x 1 where each entry can be either 0 or 1 representing the label of corresponding feature vector\n",
    "\n",
    "    Output: \n",
    "        error: the scalar value of error function of 2-class logistic regression\n",
    "        error_grad: the vector of size (D+1) x 1 representing the gradient of\n",
    "                    error function\n",
    "    \"\"\"\n",
    "    train_data, labeli = args\n",
    "\n",
    "    n_data = train_data.shape[0]\n",
    "    n_features = train_data.shape[1]\n",
    "    error = 0\n",
    "    error_grad = np.zeros((n_features + 1, 1))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    \n",
    "    \n",
    "    ################################# Start ######################################\n",
    "    \n",
    "    # added by : Zulkar : 4/18/16 2:23 pm \n",
    "    # add bias term at the beginning of the feature vector instead of the end. \n",
    "    train_data_with_bias = np.ones((n_data , n_features + 1))\n",
    "    train_data_with_bias[:,1:] = train_data  # dim : N * D+1\n",
    "    #print(\"train_data_with_bias:\")\n",
    "    #print(train_data_with_bias.shape)\n",
    "    \n",
    "    # compute theta_n = sigma(w.T,x_n)   \n",
    "    # Since , initialWeights dim = (D+1) * 1\n",
    "    #          train_data_with_bias dim = N * (D+1)\n",
    "    # train_data_with_bias . initialWeights will give dim = N * 1\n",
    "    \n",
    "    W = initialWeights.reshape((n_feature+1,1))\n",
    "    theta_n_temp = np.dot(train_data_with_bias,W)  # dim = N * 1\n",
    "    theta_n = sigmoid(theta_n_temp)\n",
    "    #print(\"theta_n:\")\n",
    "    #print (theta_n.shape)\n",
    "    one_minus_theta_n = 1 - theta_n  # dim : N * 1\n",
    "    \n",
    "    ln_theta_n = np.log(theta_n)   # dim : N * 1\n",
    "    \n",
    "    ln_one_minus_theta_n = np.log(one_minus_theta_n)  # dim N * 1\n",
    "    \n",
    "    y_n = labeli   # dim : N * 1\n",
    "    \n",
    "    one_minus_y_n = 1 - labeli   # dim : N * 1\n",
    "    \n",
    "    yn_ln_thetan = y_n * ln_theta_n   # dim : N * 1\n",
    "    \n",
    "    one_minus_yn_thetan = one_minus_y_n * ln_one_minus_theta_n  # dim : N * 1\n",
    "    \n",
    "    add_both_part = yn_ln_thetan + one_minus_yn_thetan  # dim : N * 1\n",
    "    \n",
    "    e_w = np.sum(add_both_part)   # scalar\n",
    "    error = (-1.0 / n_data) * e_w  # scalar\n",
    "    \n",
    "    #print (error)\n",
    "    # added by : Zulkar : 4/18/16 2:23 pm\n",
    "    ################################## end ###############################################\n",
    "     \n",
    "    # added by : Zulkar : 4/24/16 1:35 pm\n",
    "    ################################## start ###############################################\n",
    "    theta_n_minus_y_n = theta_n - y_n  # dim : N * 1\n",
    "    \n",
    "    # transpose the training data : \n",
    "    train_data_with_bias_transpose = np.transpose(train_data_with_bias)   # dim : (D+1) * N\n",
    "    \n",
    "    \n",
    "    sum_theta_n_minus_y_n_into_xn = np.dot(train_data_with_bias_transpose, theta_n_minus_y_n)  #(D+1)*N . N*1\n",
    "    \n",
    "    error_grad_temp = (1.0 / n_data) * sum_theta_n_minus_y_n_into_xn\n",
    "    error_grad = error_grad_temp.flatten()\n",
    "    #print(\"error_grad:\")\n",
    "    #print(error_grad.shape)\n",
    "    # added by : Zulkar : 4/24/16 1:35 pm\n",
    "    ################################## end ###############################################\n",
    "    \n",
    "\n",
    "    return error, error_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blrPredict(W, data):\n",
    "    \"\"\"\n",
    "     blrObjFunction predicts the label of data given the data and parameter W \n",
    "     of Logistic Regression\n",
    "     \n",
    "     Input:\n",
    "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight \n",
    "         vector of a Logistic Regression classifier.\n",
    "         X: the data matrix of size N x D\n",
    "         \n",
    "     Output: \n",
    "         label: vector of size N x 1 representing the predicted label of \n",
    "         corresponding feature vector given in data matrix\n",
    "\n",
    "    \"\"\"\n",
    "    label = np.zeros((data.shape[0], 1))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    # add bias term at the beginning of the feature vector instead of the end. \n",
    "    N = data.shape[0]\n",
    "    D = data.shape[1]\n",
    "    data_with_bias = np.ones(( N , D + 1))  # dim : N * (D+1)\n",
    "    data_with_bias[:,1:] = data  # dim : N * D+1\n",
    "    \n",
    "    #data_with_bias_transpose = np.transpose(data_with_bias)  # dim : (D+1) * N\n",
    "    wT_x = np.dot(data_with_bias , W) # dim :  (D+1) * N . (D + 1) x 10 = (D+1) *  10\n",
    "    \n",
    "    sigma_wT_x = sigmoid(wT_x)\n",
    "    \n",
    "    label_temp = np.argmax(sigma_wT_x, axis = 1)\n",
    "    label = np.reshape(label_temp, (N,1) )\n",
    "    \n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlrObjFunction(params, *args):\n",
    "    \"\"\"\n",
    "    mlrObjFunction computes multi-class Logistic Regression error function and\n",
    "    its gradient.\n",
    "\n",
    "    Input:\n",
    "        initialWeights: the weight vector of size (D + 1) x 1\n",
    "        train_data: the data matrix of size N x D\n",
    "        labeli: the label vector of size N x 1 where each entry can be either 0 or 1\n",
    "                representing the label of corresponding feature vector\n",
    "\n",
    "    Output:\n",
    "        error: the scalar value of error function of multi-class logistic regression\n",
    "        error_grad: the vector of size (D+1) x 10 representing the gradient of\n",
    "                    error function\n",
    "    \"\"\"\n",
    "    n_data = train_data.shape[0]\n",
    "    n_feature = train_data.shape[1]\n",
    "    error = 0\n",
    "    error_grad = np.zeros((n_feature + 1, n_class))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "\n",
    "    return error, error_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlrPredict(W, data):\n",
    "    \"\"\"\n",
    "     mlrObjFunction predicts the label of data given the data and parameter W\n",
    "     of Logistic Regression\n",
    "\n",
    "     Input:\n",
    "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight\n",
    "         vector of a Logistic Regression classifier.\n",
    "         X: the data matrix of size N x D\n",
    "\n",
    "     Output:\n",
    "         label: vector of size N x 1 representing the predicted label of\n",
    "         corresponding feature vector given in data matrix\n",
    "\n",
    "    \"\"\"\n",
    "    label = np.zeros((data.shape[0], 1))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script for Logistic Regression\n",
    "\"\"\"\n",
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "# number of classes\n",
    "n_class = 10\n",
    "\n",
    "# number of training samples\n",
    "n_train = train_data.shape[0]\n",
    "\n",
    "# number of features\n",
    "n_feature = train_data.shape[1]\n",
    "\n",
    "Y = np.zeros((n_train, n_class))\n",
    "for i in range(n_class):\n",
    "    Y[:, i] = (train_label == i).astype(int).ravel()\n",
    "\n",
    "# # Logistic Regression with Gradient Descent\n",
    "# W = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights = np.zeros((n_feature + 1, 1))\n",
    "# opts = {'maxiter': 100}\n",
    "# for i in range(n_class):\n",
    "#     labeli = Y[:, i].reshape(n_train, 1)\n",
    "#     args = (train_data, labeli)\n",
    "#     nn_params = minimize(blrObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)\n",
    "#     W[:, i] = nn_params.x.reshape((n_feature + 1,))\n",
    "\n",
    "# # Find the accuracy on Training Dataset\n",
    "# predicted_label = blrPredict(W, train_data)\n",
    "# print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label == train_label).astype(float))) + '%')\n",
    "\n",
    "# # Find the accuracy on Validation Dataset\n",
    "# predicted_label = blrPredict(W, validation_data)\n",
    "# print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label).astype(float))) + '%')\n",
    "\n",
    "# # Find the accuracy on Testing Dataset\n",
    "# predicted_label = blrPredict(W, test_data)\n",
    "# print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label == test_label).astype(float))) + '%')\n",
    "\n",
    "# \"\"\"\n",
    "# Script for Support Vector Machine\n",
    "# \"\"\"\n",
    "\n",
    "# print('\\n\\n--------------SVM-------------------\\n\\n')\n",
    "# ##################\n",
    "# # YOUR CODE HERE #\n",
    "# ##################\n",
    "\n",
    "\n",
    "# # Linear kernel\n",
    "\n",
    "\n",
    "# print('linear kernel')\n",
    "\n",
    "# clf = SVC(kernel='linear')\n",
    "# clf.fit(train_data, train_label.flatten())\n",
    "\n",
    "# train_acc = 100*clf.score(train_data, train_label)\n",
    "# print('\\n Training Accuracy:' + str(train_acc) + '%')\n",
    "\n",
    "# test_acc = 100*clf.score(test_data, test_label)\n",
    "# print('\\n Testing Accuracy:' + str(test_acc) + '%')\n",
    "\n",
    "# valid_acc = 100*clf.score(validation_data, validation_label)\n",
    "# print('\\n Validation Accuracy:' + str(valid_acc) + '%')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Radial basis function:  gamma = 1\n",
    "\n",
    "\n",
    "# print('\\n\\n Radial basis function: gamma = 1')\n",
    "\n",
    "# clf = SVC(kernel='rbf', gamma=1.0)\n",
    "# clf.fit(train_data, train_label.flatten())\n",
    "\n",
    "# train_acc = 100*clf.score(train_data, train_label)\n",
    "# print('\\n Training Accuracy:' + str(train_acc) + '%')\n",
    "\n",
    "# test_acc = 100*clf.score(test_data, test_label)\n",
    "# print('\\n Testing Accuracy:' + str(test_acc) + '%')\n",
    "\n",
    "# valid_acc = 100*clf.score(validation_data, validation_label)\n",
    "# print('\\n Validation Accuracy:' + str(valid_acc) + '%')\n",
    "\n",
    "\n",
    "# # Radial basis function: gamma = 0\n",
    "# print('\\n\\n Radial basis function: gamma = 0')\n",
    "# clf = SVC(kernel='rbf')\n",
    "# clf.fit(train_data, train_label.flatten())\n",
    "\n",
    "\n",
    "# train_acc = 100*clf.score(train_data, train_label)\n",
    "# print('\\n Training Accuracy:' + str(train_acc) + '%')\n",
    "\n",
    "# test_acc = 100*clf.score(test_data, test_label)\n",
    "# print('\\n Testing Accuracy:' + str(test_acc) + '%')\n",
    "\n",
    "# valid_acc = 100*clf.score(validation_data, validation_label)\n",
    "# print('\\n Validation Accuracy:' + str(valid_acc) + '%')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Radial basis function with C being 1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100\n",
    "# print('\\n\\n SVM with  different values of C')\n",
    "# train_accuracy = np.zeros(11)\n",
    "# test_accuracy = np.zeros(11)\n",
    "# valid_accuracy = np.zeros(11)\n",
    "\n",
    "\n",
    "# C = 1.0\n",
    "# for i in range(11):\n",
    "#     clf = SVC(C=C, kernel='rbf')\n",
    "#     clf.fit(train_data, train_label.flatten())\n",
    "    \n",
    "#     print ('\\n C is :')\n",
    "#     print (C)\n",
    "    \n",
    "#     train_accuracy[i] = 100*clf.score(train_data, train_label)\n",
    "#     print('\\n Training  Accuracy for C : ' + str(train_accuracy[i]) + '%')\n",
    "    \n",
    "#     test_accuracy[i] = 100*clf.score(test_data, test_label)\n",
    "#     print('\\n Testing  Accuracy for C :'  + str(test_accuracy[i]) + '%')\n",
    "    \n",
    "#     valid_accuracy[i] = 100*clf.score(validation_data, validation_label)\n",
    "#     print('\\n Validation  Accuracy for C :'  + str(valid_accuracy[i]) + '%')\n",
    "    \n",
    "#     if (i == 0):\n",
    "#         C = 10\n",
    "#     else:\n",
    "#         C = C + 10\n",
    "\n",
    "\n",
    "\n",
    "# # Plot accuracies\n",
    "# C_range = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "# plot(C_range, train_accuracy, 'o-',\n",
    "#     C_range, test_accuracy,'o-',\n",
    "#     C_range, valid_accuracy, 'o-')\n",
    "\n",
    "# ylabel('Accuracy (%)')\n",
    "# xlabel('Values of C')\n",
    "\n",
    "# title('Accuracy using SVM and different values of C')\n",
    "# legend(('Training','Test', 'Validation'), loc='lower right')\n",
    "# grid(True)\n",
    "# savefig(\"C.png\")\n",
    "# show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_dot_x.shape:  (50000, 10)\n",
      "sigmoid_w_dot_x.shape:  (50000, 10)\n",
      "sum_sigmoid_w_dot_x.shape:  (50000,)\n",
      "inv_sum_sigmoid_w_dot_x.shape:  (50000,)\n",
      "posterior_probability.shape:  (50000, 10)\n",
      "w_dot_x.shape:  (50000, 10)\n",
      "sigmoid_w_dot_x.shape:  (50000, 10)\n",
      "sum_sigmoid_w_dot_x.shape:  (50000,)\n",
      "inv_sum_sigmoid_w_dot_x.shape:  (50000,)\n",
      "posterior_probability.shape:  (50000, 10)\n",
      "\n",
      " Training set Accuracy:9.846%\n",
      "\n",
      " Validation set Accuracy:10.0%\n",
      "\n",
      " Testing set Accuracy:9.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:127: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "# def one_of_k(labels,k):\n",
    "#     # inputs : labels : the label vector that needs one of k encoding. dimension : N * 1 \n",
    "#     #          k : in our case k = 10\n",
    "    \n",
    "#     N = labels.shape[0]\n",
    "\n",
    "#     # create an array of size N * k with all zeros\n",
    "#     result = np.zeros( (N , k) )\n",
    "    \n",
    "#     # forcing labels to be integer:\n",
    "#     int_labels = labels.astype(int)\n",
    "    \n",
    "#     row_index = 0\n",
    "#     for index in int_labels:\n",
    "#         result[row_index,index] = 1\n",
    "#         row_index = row_index + 1\n",
    "#     return result\n",
    "\n",
    "\n",
    "\n",
    "# FOR EXTRA CREDIT ONLY\n",
    "def mlrObjFunction(params, *args):\n",
    "    \"\"\"\n",
    "    mlrObjFunction computes multi-class Logistic Regression error function and\n",
    "    its gradient.\n",
    "\n",
    "    Input:\n",
    "        initialWeights: the weight vector of size (D + 1) x 1\n",
    "        train_data: the data matrix of size N x D\n",
    "        labeli: the label vector of size N x 1 where each entry can be either 0 or 1\n",
    "                representing the label of corresponding feature vector\n",
    "\n",
    "    Output:\n",
    "        error: the scalar value of error function of multi-class logistic regression\n",
    "        error_grad: the vector of size (D+1) x 10 representing the gradient of\n",
    "                    error function\n",
    "    \"\"\"\n",
    "    n_data = train_data.shape[0]\n",
    "    n_feature = train_data.shape[1]\n",
    "    error = 0\n",
    "    error_grad = np.zeros((n_feature + 1, n_class))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    \n",
    "    # args gives access to all of these variables - train_data, n_train, n_class\n",
    "    # args = (train_data, Y)\n",
    "    # Y = ((n_train, n_class))\n",
    "    \n",
    "    \n",
    "    # code var name  = variable shape\n",
    "    \n",
    "    # initialWeights           = (D + 1) x 1   (716,1)\n",
    "    # initialWeights_b         = (D + 1) x 1   (716,10)\n",
    "    # initialWeights_transpose =  1 x (D + 1)  (1,716)\n",
    "    # train_data               =  N x D        (50000,715)\n",
    "    # train_data_bias          =  N x (D + 1)  (50000,716)\n",
    "    # w_dot_x                  =               (50000, 1)\n",
    "    # sigma_w_dot_x            =               (50000, 1)\n",
    "    # posterior_probability    =               (50000, 1)\n",
    "    # labeli                   =  N x 1\n",
    "    # error                    =  1 x 1\n",
    "    # error_grad               = (D + 1) x 10\n",
    "    \n",
    "    # N = 50000\n",
    "    # K = \n",
    "    \n",
    "    # n_train                  = 50000\n",
    "    # n_class                  = 10\n",
    "    \n",
    "        \n",
    "# target vector yn\n",
    "# feature vector xn\n",
    "# class Ck\n",
    "# element k, which equals one\n",
    "# Y is an N Ã— K matrix (obtained using 1-of-K encoding) of target variables with elements ynk\n",
    "    \n",
    "#     Please refer to your first assignment. This is same as the \n",
    "#     1-of-K encoding that you would have done there. ynk is the value \n",
    "#     for the Y[n][k] entry in the matrix.\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Formula 5 : Posterior_Probabilities\n",
    "    train_data_bias = np.insert(train_data, 715, 1, axis = 1)                           #(50000,716)\n",
    "#     print(\"train_data_bias: \", train_data_bias)\n",
    "    w_dot_x = np.dot(train_data_bias,initialWeights_b)          #wTx                    #(50000,10)\n",
    "    print(\"w_dot_x.shape: \", w_dot_x.shape)\n",
    "#     print(\"w_dot_x: \", w_dot_x)\n",
    "    sigmoid_w_dot_x = sigmoid(w_dot_x)                          #exp(wTx)               #(50000,10)\n",
    "    print(\"sigmoid_w_dot_x.shape: \", sigmoid_w_dot_x.shape)\n",
    "#     print(\"sigmoid_w_dot_x: \", sigmoid_w_dot_x)\n",
    "    sum_sigmoid_w_dot_x = np.sum(sigmoid_w_dot_x,axis = 1)      #sum(exp(wTx))          #(50000,1)\n",
    "    print(\"sum_sigmoid_w_dot_x.shape: \", sum_sigmoid_w_dot_x.shape)\n",
    "#     print(\"sum_sigmoid_w_dot_x: \", sum_sigmoid_w_dot_x)\n",
    "    inv_sum_sigmoid_w_dot_x = 1.0 / sum_sigmoid_w_dot_x                                 #(50000,1)\n",
    "    print(\"inv_sum_sigmoid_w_dot_x.shape: \", inv_sum_sigmoid_w_dot_x.shape)\n",
    "#     print(\"inv_sum_sigmoid_w_dot_x: \", inv_sum_sigmoid_w_dot_x)\n",
    "\n",
    "    posterior_probability = np.zeros((sigmoid_w_dot_x.shape[0], sigmoid_w_dot_x.shape[1]))\n",
    "\n",
    "    for i in range(sigmoid_w_dot_x.shape[0]):        #50000\n",
    "        for k in range(sigmoid_w_dot_x.shape[1]): \n",
    "            posterior_probability[i][k] = sigmoid_w_dot_x[i][k] * inv_sum_sigmoid_w_dot_x[i]\n",
    "            \n",
    "\n",
    "#     posterior_probability = np.dot(sigmoid_w_dot_x, inv_sum_sigmoid_w_dot_x) # =theta_nk  #(50000,1)\n",
    "    print(\"posterior_probability.shape: \", posterior_probability.shape)\n",
    "    print(\"posterior_probability: \", posterior_probability)\n",
    "    \n",
    "    \n",
    "#     print(\"posterior_probability: \", posterior_probability)\n",
    "    \n",
    "    # Formula 6 : likelihood\n",
    "    y_nk = one_of_k(posterior_probability,n_class)      #(50000,10)\n",
    "#     print(\"y_nk: \", y_nk)\n",
    "    power_pp_ynk = np.power(posterior_probability,y_nk) #(50000,10)\n",
    "#     print(\"power_pp_ynk: \", power_pp_ynk)\n",
    "    k_product = np.prod(power_pp_ynk,axis = 1)          #(50000,1)\n",
    "#     print(\"k_product: \", k_product)\n",
    "    likelihood = np.prod(k_product,axis = 0)            #scalar\n",
    "#     print(\"likelihood: \", likelihood)\n",
    "    \n",
    "    # Formula 7 : log likelihood (error)\n",
    "    error = -1.0 * np.log(likelihood)      #scalar\n",
    "#     print(\"error: \", error)\n",
    "    \n",
    "    # Formula 8 : gradient of error function\n",
    "    \n",
    "    difference = posterior_probability - y_nk;              #(50000,10)\n",
    "    transpose_difference = np.transpose(difference)         #(10,50000)\n",
    "    product = np.dot(transpose_difference,train_data_bias)  #(10,716)\n",
    "    error_grad = np.sum(product)                            #scalar\n",
    "    \n",
    "    # Formula 9 : (not needed or used in the assignment)\n",
    "    \n",
    "    \n",
    "\n",
    "#     print(gradient_error_function.shape)\n",
    "#     print(gradient_error_function)\n",
    "\n",
    "    return error, error_grad\n",
    "\n",
    "def mlrPredict(W, data):\n",
    "    \"\"\"\n",
    "     mlrObjFunction predicts the label of data given the data and parameter W\n",
    "     of Logistic Regression\n",
    "\n",
    "     Input:\n",
    "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight\n",
    "         vector of a Logistic Regression classifier.\n",
    "         X: the data matrix of size N x D\n",
    "\n",
    "     Output:\n",
    "         label: vector of size N x 1 representing the predicted label of\n",
    "         corresponding feature vector given in data matrix\n",
    "\n",
    "    \"\"\"\n",
    "    label = np.zeros((data.shape[0], 1))   #(50000, 1) \n",
    "    \n",
    "    # data - train_data or validation_data or test_data = N x D\n",
    "    \n",
    "    #add bias to data\n",
    "    #multiply W x data\n",
    "    #take the highest value of the 10 for each of the 50000 entries and return that index\n",
    "\n",
    "    # W        (716, 10)\n",
    "    # data     (50000, 715)\n",
    "    \n",
    "    data_bias = np.insert(data, 715, 1, axis = 1)    #(50000, 716)\n",
    "    dot_product = np.dot(data_bias,W)                      #(50000, 10)\n",
    "     \n",
    "    for i in range(dot_product.shape[0]):        #50000\n",
    "        max_value = 0.0\n",
    "        max_index = 0.0\n",
    "        \n",
    "        for k in range(dot_product.shape[1]):    #10\n",
    "            if dot_product[i][k] > max_value:\n",
    "                max_value = dot_product[i][k]\n",
    "                max_index = k\n",
    "                \n",
    "        label[i] = max_index\n",
    "    \n",
    "#     print(data_bias.shape)\n",
    "#     print(data_bias)\n",
    "\n",
    "    return label\n",
    "\n",
    "\"\"\"\n",
    "Script for Extra Credit Part\n",
    "\"\"\"\n",
    "# FOR EXTRA CREDIT ONLY\n",
    "W_b = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights_b = np.zeros((n_feature + 1, n_class))\n",
    "opts_b = {'maxiter': 100}\n",
    "\n",
    "args_b = (train_data, Y)\n",
    "nn_params = minimize(mlrObjFunction, initialWeights_b, jac=True, args=args_b, method='CG', options=opts_b)\n",
    "W_b = nn_params.x.reshape((n_feature + 1, n_class))\n",
    "\n",
    "# Find the accuracy on Training Dataset\n",
    "predicted_label_b = mlrPredict(W_b, train_data)\n",
    "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label_b == train_label).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Validation Dataset\n",
    "predicted_label_b = mlrPredict(W_b, validation_data)\n",
    "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label_b == validation_label).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Testing Dataset\n",
    "predicted_label_b = mlrPredict(W_b, test_data)\n",
    "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label_b == test_label).astype(float))) + '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_bias:  [[ 0.  0.  0. ...,  0.  0.  1.]\n",
    " [ 0.  0.  0. ...,  0.  0.  1.]\n",
    " [ 0.  0.  0. ...,  0.  0.  1.]\n",
    " ..., \n",
    " [ 0.  0.  0. ...,  0.  0.  1.]\n",
    " [ 0.  0.  0. ...,  0.  0.  1.]\n",
    " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
    "w_dot_x:  [[ 0.  0.  0. ...,  0.  0.  0.]\n",
    " [ 0.  0.  0. ...,  0.  0.  0.]\n",
    " [ 0.  0.  0. ...,  0.  0.  0.]\n",
    " ..., \n",
    " [ 0.  0.  0. ...,  0.  0.  0.]\n",
    " [ 0.  0.  0. ...,  0.  0.  0.]\n",
    " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
    "sigmoid_w_dot_x:  [[ 0.5  0.5  0.5 ...,  0.5  0.5  0.5]\n",
    " [ 0.5  0.5  0.5 ...,  0.5  0.5  0.5]\n",
    " [ 0.5  0.5  0.5 ...,  0.5  0.5  0.5]\n",
    " ..., \n",
    " [ 0.5  0.5  0.5 ...,  0.5  0.5  0.5]\n",
    " [ 0.5  0.5  0.5 ...,  0.5  0.5  0.5]\n",
    " [ 0.5  0.5  0.5 ...,  0.5  0.5  0.5]]\n",
    "sum_sigmoid_w_dot_x:  250000.0\n",
    "inv_sum_sigmoid_w_dot_x:  4e-06\n",
    "posterior_probability:  [[  2.00000000e-06   2.00000000e-06   2.00000000e-06 ...,   2.00000000e-06\n",
    "    2.00000000e-06   2.00000000e-06]\n",
    " [  2.00000000e-06   2.00000000e-06   2.00000000e-06 ...,   2.00000000e-06\n",
    "    2.00000000e-06   2.00000000e-06]\n",
    " [  2.00000000e-06   2.00000000e-06   2.00000000e-06 ...,   2.00000000e-06\n",
    "    2.00000000e-06   2.00000000e-06]\n",
    " ..., \n",
    " [  2.00000000e-06   2.00000000e-06   2.00000000e-06 ...,   2.00000000e-06\n",
    "    2.00000000e-06   2.00000000e-06]\n",
    " [  2.00000000e-06   2.00000000e-06   2.00000000e-06 ...,   2.00000000e-06\n",
    "    2.00000000e-06   2.00000000e-06]\n",
    " [  2.00000000e-06   2.00000000e-06   2.00000000e-06 ...,   2.00000000e-06\n",
    "    2.00000000e-06   2.00000000e-06]]\n",
    "y_nk:  [[ 1.  0.  0. ...,  0.  0.  0.]\n",
    " [ 1.  0.  0. ...,  0.  0.  0.]\n",
    " [ 1.  0.  0. ...,  0.  0.  0.]\n",
    " ..., \n",
    " [ 1.  0.  0. ...,  0.  0.  0.]\n",
    " [ 1.  0.  0. ...,  0.  0.  0.]\n",
    " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
    "power_pp_ynk:  [[  2.00000000e-06   1.00000000e+00   1.00000000e+00 ...,   1.00000000e+00\n",
    "    1.00000000e+00   1.00000000e+00]\n",
    " [  2.00000000e-06   1.00000000e+00   1.00000000e+00 ...,   1.00000000e+00\n",
    "    1.00000000e+00   1.00000000e+00]\n",
    " [  2.00000000e-06   1.00000000e+00   1.00000000e+00 ...,   1.00000000e+00\n",
    "    1.00000000e+00   1.00000000e+00]\n",
    " ..., \n",
    " [  2.00000000e-06   1.00000000e+00   1.00000000e+00 ...,   1.00000000e+00\n",
    "    1.00000000e+00   1.00000000e+00]\n",
    " [  2.00000000e-06   1.00000000e+00   1.00000000e+00 ...,   1.00000000e+00\n",
    "    1.00000000e+00   1.00000000e+00]\n",
    " [  2.00000000e-06   1.00000000e+00   1.00000000e+00 ...,   1.00000000e+00\n",
    "    1.00000000e+00   1.00000000e+00]]\n",
    "k_product:  [  2.00000000e-06   2.00000000e-06   2.00000000e-06 ...,   2.00000000e-06\n",
    "   2.00000000e-06   2.00000000e-06]\n",
    "likelihood:  0.0\n",
    "error:  inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
